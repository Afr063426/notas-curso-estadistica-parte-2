\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Notas de Clase CA-403},
  pdfauthor={Maikol Solís},
  pdfborder={0 0 0},
  breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

%\usepackage{inputenc}
% \usepackage{newpxtext,newpxmath}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage[spanish]{babel}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{amsmath, amsthm, amssymb,amsbsy}
\usepackage{mathtools}
\usepackage{graphics, graphicx}
% \usepackage{setspace}
% \doublespacing
%\addbibresource{bibliografia.bib}


% \usepackage{tcolorbox}
% \tcbuselibrary{theorems}
% \tcbuselibrary{breakable}
% 
% \newtcbtheorem[number within=section]{nota}{Nota}%
% {breakable, colback=yellow!5, colframe=yellow!40!gray,
% 	fonttitle=\bfseries}{nota}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{cuidado}{Cuidado}%
% {breakable, colback=red!5, colframe=red!50!gray,
% 	fonttitle=\bfseries}{cuidado}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{tarea}{Tarea}%
% {breakable, colback=blue!5, colframe=blue!35!black,
% 	fonttitle=\bfseries}{tarea}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{solucion}{Solución}%
% {breakable, colback=gray!5, colframe=gray!35!black,
% 	fonttitle=\bfseries}{sol}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{pregunta}{Pregunta}%
% {breakable,  colback=green!5, colframe=green!35!black,
% 	fonttitle=\bfseries}{preg}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{ejemplo}{Ejemplo}%
% {breakable, colback=magenta!10, colframe=magenta!50!black,
% 	fonttitle=\bfseries}{ej}
% 
% \newtcbtheorem[number within=section,use counter
% from=nota]{laboratorio}{Laboratorio}%
% {breakable, colback=purple!10, colframe=purple!50!black,
% 	fonttitle=\bfseries}{lab}
%%end novalidate

%
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amssymb}
%%%% DEFINICIÓN DE ESTILOS DE TEOREMAS %%%
%\theoremstyle{definition}
%\newtheorem{definicion}{Definición}
%
%\theoremstyle{plain}
%\newtheorem{teorema}{Teorema}
%\newtheorem{lema}{Lema}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[style=authoryear,]{biblatex}
\addbibresource{bibliografia.bib}

\title{Notas de Clase CA-403}
\author{Maikol Solís}
\date{}

\usepackage{amsthm}
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{lemma}{Lema}[chapter]
\newtheorem{corollary}{Corolario}[chapter]
\newtheorem{proposition}{Proposición}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definición}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Ejemplo}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Ejercicio}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Nota: }
\newtheorem*{solution}{Solución}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{introducciuxf3n}{%
\chapter{Introducción}\label{introducciuxf3n}}

\hypertarget{estimaciuxf3n-de-densidades}{%
\chapter{Estimación de densidades}\label{estimaciuxf3n-de-densidades}}

\hypertarget{histograma}{%
\section{Histograma}\label{histograma}}

El histograma es una de las estructuras básicas en estadística. Básicamente con este objeto se puede visualizar la distribución de los datos sin tener conocimiento previo de los mismos.

\hypertarget{construcciuxf3n-estaduxedstica}{%
\subsection{Construcción Estadística}\label{construcciuxf3n-estaduxedstica}}

Suponga que \(X_1,X_2, \dots ,X_n\) proviene de una distribución desconocida.

\begin{itemize}
\item
  Seleccione un origen \(x_0\) y divida la linea real en \emph{segmentos}.
  \begin{equation*}
  B_j = [x_0 +(j - 1)h,x_0 + jh) \quad j\in \mathbb{Z}
  \end{equation*}
\item
  Cuente cuántas observaciones caen en cada segmento. \(n_j\).
\end{itemize}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/observaciones-histograma-1.pdf}
- Cuente la frecuencia por el tamaño de muestra \(n\) y el ancho de banda \(h\).
\begin{equation*}
f_j = \frac{n_j}{nh}
\end{equation*}

\begin{itemize}
\tightlist
\item
  Dibuje el histograma.
\end{itemize}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/ejemplo-inicial-histograma-1.pdf}

Formalmente el histograma es el

\begin{equation*}
\hat{f}_h(x) = \frac{1}{nh} \sum_{i = 1}^{n} \sum_{j} I(X_i\in B_j) I(x\in B_j),
\end{equation*}

donde \(I\) es la indicadora.

\hypertarget{construcciuxf3n-probabilistica}{%
\subsection{Construcción probabilistica}\label{construcciuxf3n-probabilistica}}

Denote \(m_j=jh-h/2\) el centro del segmento,

\begin{align*}
\mathbb{P}\left(X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2} \right)\right) & =
\int_{m_j - \frac{h}{2}}^{m_j + \frac{h}{2}} f(u)du                                             \\
& \approx f(m_j)h
\end{align*}

Esto se puede aproximar como

\begin{equation*}
\mathbb{P} \left(X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right)  \approx   \frac{1}{n} \#
\left\{X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right\}
\end{equation*}

Acomodando un poco la expresión

\begin{equation*}
\hat{f}_h(m_j) =  \frac{1}{nh} \#
\left\{X\in \left[m_j - \frac{h}{2},m_j + \frac{h}{2}\right) \right\}
\end{equation*}

\hypertarget{propiedades-estaduxedsticas}{%
\subsection{Propiedades estadísticas}\label{propiedades-estaduxedsticas}}

Suponga que \(x_0 = 0\) y que \(x \in B_j\) fijo, entonces

\begin{equation*}
\hat{f}_h(m_j) =  \frac{1}{nh} \sum_{i = 1}^{n} I(X_i \in B_j)
\end{equation*}

\hypertarget{sesgo}{%
\subsection{Sesgo}\label{sesgo}}

El cálculo del sesgo es el

\begin{align*}
\mathbb{E}\left[ \hat{f}_h(m_j)\right]
& =  \frac{1}{nh} \sum_{i = 1}^{n} \mathbb{E}\left[ I(X_i \in B_j)\right] \\
& = \frac{1}{nh} n \mathbb{E}\left[ I(X_i \in B_j)\right]
\end{align*}

\(I(X_i \in B_j)\) es una indicadora con probabilidad de 1 de \(\int_{(j - 1)h}^{jh} f(u)du\) y 0 sino.

Entonces

\begin{align*}
\mathbb{E}\left[ I(X_i \in B_j)\right] = \mathbb{P}\left(I(X_i \in
B_j)=1\right) = \int_{(j - 1)h}^{jh} f(u)du.
\end{align*}

Entonces,
\begin{align*}
\mathbb{E}\left[{f}_h(m_j)\right]
& = \frac{1}{h} \int_{(j - 1)h}^{jh} f(u)du
\end{align*}

\begin{equation*}
Sesgo(\hat{f}_h(m_j)) = \frac{1}{h} \int_{(j -
1)h}^{jh} f(u)du - f(x)
\end{equation*}

Esto se puede aproximar usando Taylor alrededor del centro \(m_j = jh - h/2\) de \(B_j\) de modo que \(f(u) - f(x) \approx f^{\prime}(m_j)(u - x)\).

\begin{equation*}
Sesgo(\hat{f}_h(m_j)) =  \frac{1}{h} \int_{(j -
1)h}^{jh} f(u) - f(x) du \approx f^\prime(m_j)(m_j - x)
\end{equation*}

\hypertarget{varianza}{%
\subsection{Varianza}\label{varianza}}

Dado que todos los \(X_i\) son i.i.d., entonces

\begin{align*}
\mathrm{Var}\left( \hat{f}_h(m_j)\right) & =
\mathrm{Var}\left( \frac{1}{nh} \sum_{i = 1}^{n} I(X_i \in B_j)\right)                                  \\
& = \frac{1}{n^2h^2} n\mathrm{Var}\left( I(X_i \in B_j)\right)
\end{align*}

La variable \(I\) es una bernoulli con parametro \(\int_{(j - 1)h}^{h} f(u)du\) por lo tanto su varianza es el

\begin{equation*}
\mathrm{Var}\left( \hat{f}_h(x)\right)\, =
\frac{1}{nh^2} \left(\int_{(j - 1)h}^{h} f(u)du \right)\left( 1 -\int_{(j - 1)h}^{h} f(u)du \right)
\end{equation*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-3}{}{\label{exr:unnamed-chunk-3} }Usando un desarrollo de Taylor como en la parte anterior, pruebe que:
\begin{equation*}
\mathrm{Var}\left( \hat{f}_h(x)\right)\approx
\frac{1}{nh} f(x)
\end{equation*}
\EndKnitrBlock{exercise}

\hypertarget{error-cuadruxe1tico-medio}{%
\subsection{Error cuadrático medio}\label{error-cuadruxe1tico-medio}}

El error cuadrático medio del histograma es el

\begin{equation*}
\mathrm{MSE}\left( \hat{f}_h(x)\right) =
\mathrm{E}\left[\left(\hat{f}_h(x) - f(x)\right)^2\right] = \mathrm{Sesgo}^2\left( \hat{f}_h(x)\right) + \mathrm{Var}\left( \hat{f}_h(x)\right).
\end{equation*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-4}{}{\label{exr:unnamed-chunk-4} }¿Pueden probar la segunda igualdad de la expresión anterior?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solución. } \fi{}Prueba segunda igualdad:
\begin{align*}
& \text{Sesgo}^2\left(\hat{f}_h(x)  \right) + \text{Var}\left( \hat{f}_h(x)\right)  = \\ & \left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 + E\left[\left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2\right] \ =
\\ & E\left[\left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 + \left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2   \right] \ \textcolor{red}{(*)} \
\end{align*}
Ahora note que:
\begin{align*}
& E\left[\left( E\left(\hat{f}_h(x)\right) - f(x)   \right) \left(E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)    \right)    \right] \ = \                                      \\
& E\left[E\left(\hat{f}_h(x)\right)^2 \right] \ - \ E\left[E\left(\hat{f}_h(x)\right)\cdot \hat{f}_h(x) \right] \ - \ E\left[f(x)\cdot E\left(\hat{f}_h(x)\right)\right] \ + \\
& E\left[f(x)\cdot \hat{f}_h(x)\right]\ = \                                                                                                                                  \\
& E\left(\hat{f}_h(x)\right)^2  \ - \ E\left(\hat{f}_h(x)\right)^2  \ - \ E\left(\hat{f}_h(x)\right)\cdot E\left( f(x)\right) \ + \
E\left( f(x)\right)\cdot E\left(\hat{f}_h(x)\right) \                                                                                                                         \\
& = 0
\end{align*}
Entonces:
\begin{align*}
& \textcolor{red}{(*)} \ = \ E\left[\left[ E\left(\hat{f}_h(x)\right) - f(x)\right]^2 \ -  \right.                                                                                                         \\
& \left. \ 2\left( E\left(\hat{f}_h(x)\right) - f(x)   \right) \left(E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)    \right) \ + \ \left( E\left(\hat{f}_h(x)\right) - \hat{f}_h(x)\right)^2   \right] \ = \  \\
& E\left[ \left(E\left(\hat{f}_h(x)\right) - f(x) \ - \ E\left(\hat{f}_h(x)\right) + \hat{f}_h(x) \right)^2   \right] \ = \                                                                                \\
& E\left[\left(\hat{f}_h(x) - f(x)\right)^2    \right]
\end{align*}
\qed
\EndKnitrBlock{solution}

Retomando los términos anteriores se tiene que

\begin{multline*}
\mathrm{MSE}\left( \hat{f}_h(x)\right) =
\frac{1}{nh} f(x) + f^\prime
\left\{
\left(
j - \frac{1}{2}
\right) h
\right\}^2
\left\{
\left(
j - \frac{1}{2}
\right) h - x
\right\}^2 \\
+ o\left(h \right) +        o\left(\frac{1}{nh} \right)
\end{multline*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Si \(h \to 0\) y \(nh \to \infty\) entonces \(\mathrm{MSE}\left( \hat{f}_h(x)\right) \to 0\). Es decir, conforme usamos más observaciones, pero el ancho de banda de banda no decrece tan rápida, entonces el error cuadrático medio converge a 0.

Esto indica que si \(\mathrm{MSE}\left( \hat{f}_h(x)\right) \to 0\) (convergencia en \(\mathbb{L}^2\)) implica que \(\hat{f}_h(x) \stackrel{\mathcal{P}}{\to} f(x)\), por lo tanto \(\hat{f}_h\) es consistente.
\EndKnitrBlock{remark}

La fórmula anterior tiene la siguiente particularidad

\begin{itemize}
\tightlist
\item
  Si \(h\to 0\), la varianza crece (converge a \(\infty\)) y el sesgo decrece (converge a \(f^\prime (0)x^2\)).
\item
  Si \(h\to \infty\), la varianza decrece (hacia 0) y el sesgo crece (hacia \(\infty\))
\end{itemize}

Note que la figura siguiente tiene esa propiedad.

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/MSE-histograma-1.pdf}

\hypertarget{error-cuadruxe1tico-medio-integrado}{%
\subsection{Error cuadrático medio integrado}\label{error-cuadruxe1tico-medio-integrado}}

El problema con el \(\mathrm{MSE}\left( \hat{f}_h(x)\right)\) es que depende completamente del punto escogido \(x\).

La solución a esto es integrar el MSE.

\begin{align*}
\mathrm{MISE}\left(  \hat{f}_h(x)\right)
& = \mathrm{E}\left[
\int_{ -\infty}^{\infty} \left\{
\hat{f}_h(x) - f(x)
\right\}^2 dx
\right]                                                       \\
& = \int_{ -\infty}^{\infty} \mathrm{E}\left[
\left\{
\hat{f}_h(x) - f(x)
\right\}^2
\right] dx                                                    \\
& = \int_{ -\infty}^{\infty}\mathrm{MSE}(\hat{f}_h(x)) \, dx
\end{align*}

Además,

\begin{align*}
\mathrm{MISE} (\hat{f}_h(x))
& = \int_{ -\infty}^{\infty} \frac{1}{nh} f(x)dx                                                                                                                                          \\
& + \int_{ -\infty}^{\infty}\, \sum_{j}^{} I(x\in B_j) \left\{ \left( j- \frac{1}{2} \right)h -x  \right\}^2 \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2 dx \\
& = \frac{1}{nh} + \sum_{j}^{} \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2 \int_{ B_j}    \left\{ \left( j- \frac{1}{2} \right)h -x  \right\}^2 dx          \\
& =\frac{1}{nh} + \frac{h^2}{12} \sum_{j} \left [f^\prime \left( \left\{j - \frac{1}{2}\right\}h \right)  \right]^2                                                                       \\
& \approx \frac{1}{nh} + \frac{h^2}{12} \int \{f^\prime(x)\}^2 dx                                                                                                                         \\
& =\frac{1}{nh} + \frac{h^2}{12} \Vert f^\prime\Vert_{2}^2
\end{align*}

\hypertarget{ancho-de-banda-uxf3ptimo-para-el-histograma}{%
\subsection{Ancho de banda óptimo para el histograma}\label{ancho-de-banda-uxf3ptimo-para-el-histograma}}

El MISE tiene el mismo comportamiento que el MSE. La figura siguiente presenta el comportamiento de la varianza, sesgo y MISE para nuestro ejemplo.

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/MISE-histograma-1.pdf}

La mala elección del parámetro \(h\) causa que el histograma no capture toda la estructura de los datos.

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-8-1.pdf}

En este caso se puede simplemente minimizar el MISE de la forma usual,

\begin{equation*}
\frac{\partial \mathrm{MISE}(f_{h})}{\partial h} = -\frac{1}{nh^2} + \frac{1}{6} h \Vert f^\prime\Vert_{2}^2 = 0
\end{equation*}

implica que

\begin{equation*}
h_{opt} = \left(\frac{6}{n\Vert f^\prime\Vert_{2}^2}\right) ^{1/3} = O\left( n^{1/3} \right).
\end{equation*}

y que por lo tanto

\begin{equation*}
\mathrm{MISE}(\hat{f}_{h}) = \frac{1}{n} \left(\frac{n\Vert f^\prime\Vert_{2}^2}{6}\right)  ^{1/3}
\end{equation*}

\BeginKnitrBlock{remark}[Recuerde de Estadística I]
\iffalse{} {Nota (Recuerde de Estadística I): } \fi{}Si \(X_1, \ldots, X_2 \sim f_{\theta}\) i.i.d, con \(\mathrm{Var}(X) = \sigma^2\), recuerde que el estimador \(\hat{\theta}\) de \(\theta\) tiene la característica que

\begin{equation*}
\mathrm{MSE}(\theta) = \mathrm{Var}(\hat{\theta}) +
\mathrm{Sesgo}^2(\hat{\theta}) = \frac{\sigma^2}{n}
\end{equation*}
\EndKnitrBlock{remark}

Según la nota anterior la tasas de convergencia del histograma es más lenta que la de un estimador parámetrico considerando la misma cantidad de datos.

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-10-1.pdf}

Finalmente, podemos encontrar el valor óptimo de esta datos dado por \(h=\)\texttt{h\_opt\_MISE}
\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-12-1.pdf}

\hypertarget{estimaciuxf3n-no-paramuxe9trica-de-densidad}{%
\section{Estimación No-paramétrica de densidad}\label{estimaciuxf3n-no-paramuxe9trica-de-densidad}}

\hypertarget{primera-construcciuxf3n}{%
\subsection{Primera construcción}\label{primera-construcciuxf3n}}

Sea \(X_{1},\ldots,X_{n}\) variables aleatorias i.i.d. con distribución \(f\) en \(\mathbb{R}\).

La distribución de \(f\) es \(F(x)=\int_{-\infty}^{x}f(t)dt\).

Considere la distribución empírica como
\[
F_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x).
\]

Por la ley de los grandes números tenemos que \(\hat{F}_{n}(x) \xrightarrow{c.s} F(x)\) para todo \(x\) en \(\mathbb{R}\)as
\(n\rightarrow\infty\). Entonces, \(F_{n}(x)\) es consistente

para todo \(x\) in \(\mathbb{R}\).

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Podríamos derivar \(\hat{F}_n\) para encontrar el estimar \(\hat{f}_n\)?
\EndKnitrBlock{remark}

La respuesta es si (más o menos).

Suponga que \(h>0\) tenemos la aproximación
\[
f(x)\approx\frac{F(x+h)-F(x-h)}{2h}.
\]

Remplazando \(F\) por su estimador \(\hat{F}_{n}\), defina
\[
\hat{f}_{n}^{R}(x)=\frac{F_{n}(x+h)-F_{n}(x-h)}{2h},
\]
donde \(\hat{f}_{n}^{R}(x)\) es el estimador de \emph{Rosenblatt }.

Podemos rescribirlo de la forma,
\[
\hat{f}_{n}^{R}(x)=\frac{1}{2nh}\sum_{i=1}^{n}I(x-h<X_{i}\leq x+h)=\frac{1}{nh}\sum_{i=1}^{n}K_{0}\left(\frac{X_{i}-x}{h}\right)
\]
con \(K_{0}(u)=\frac{1}{2}I(-1<u\leq1)\), lo cuál es equivalente al caso del histograma.

\hypertarget{otra-construcciuxf3n}{%
\subsection{Otra construcción}\label{otra-construcciuxf3n}}

Con el histograma construimos una serie de segmentos fijo \(B_{j}\) y contabamos el número de datos que estaban \textbf{CONTENIDOS en \(B_{j}\)}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Qué pasaría si cambiamos la palabra \textbf{CONTENIDOS} por \textbf{ALREDEDOR DE \enquote{x}}?
\EndKnitrBlock{remark}

Suponga que se tienen intervalos de longitud \$ 2h \$, es decir, intervalos de la forma \$ {[}x-h,x+h) \$.

El histograma se escribe como

\begin{equation*}
\hat{f_{h}}(x) = \dfrac{1}{2hn} \# \{ X_i \in [x-h,x+h) \}.
\end{equation*}

Ahora tratemos de modificar ligeramente esta expresión notando dos cosas

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{equation*}
  \frac{1}{2} I \left( \left\vert u \right\vert \leq 1 \right)
  \end{equation*}
  con \(u = \frac{x-xi}{h}\)
\item
\end{enumerate}

\begin{equation*}
\frac{1}{2}\# \{ X_i \in [x-h,x+h) \}
=\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
=\sum_{i=1}^{n}  \frac{1}{2} I \left( \left\vert \frac{x-x_{i}}{h}
\right\vert \leq 1 \right)
\end{equation*}
\textbackslash{}end\{enumerate\}

Finalmente se tiene que

\begin{equation*}
\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
\end{equation*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Qué pasaría si cambiaríamos la función \(K\) del histograma por una más general?
\EndKnitrBlock{remark}

Esta función debería cumplir las siguientes características

\begin{itemize}
\tightlist
\item
  \(K(u)\geq 0\).
\item
  \(\int_{-\infty}^{\infty} K(u)du = 1\).
\item
  \(\int_{-\infty}^{\infty} u K(u)du = 0\).
\item
  \(\int_{-\infty}^{\infty} u^{2} K(u)du <\infty\).
\end{itemize}

Por ejemplo:

\begin{itemize}
\tightlist
\item
  \textbf{Uniforme:} \(\frac{1}{2} I \left( \left\vert u \right\vert \leq 1 \right)\).
\item
  \textbf{Triangular:} \((1-|u|) I \left( \left\vert u \right\vert \leq 1 \right)\).
\item
  \textbf{Epanechnikov:} \(\frac{3}{4} (1-u^{2}) I \left( \left\vert u \right\vert \leq 1 \right)\).
\item
  \textbf{Gausian:} \(\frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2}u^{2} \right)\).
\end{itemize}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-16-1.pdf}

Entonces se tendría que la expresión general para un estimador por núcleos es

\begin{equation*}
\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-x_{i}}{h} \right)
\end{equation*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Qué pasaría si modificamos el ancho de banda \(h\) para un mismo kernel?
\EndKnitrBlock{remark}

Nuevamente sería el ancho de banda ya que

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-18-1.pdf}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Qué pasaría si modificamos el kernel para un mismo ancho de banda \(h\)?
\EndKnitrBlock{remark}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-20-1.pdf}

Recordemos nuevamente la fórmula

\begin{equation*}
\hat{f}_{h}\left( x \right) = \frac{1}{nh}\sum_{i=1}^{n} K\left( \frac{x-X_{i}}{h} \right)
\end{equation*}

Cada sumando de esta expresión es una función por si misma. Si la integramos se obtiene que

\begin{equation*}
\frac{1}{nh}\int K\left( \frac{x-X_{i}}{h} \right) dx
= \frac{1}{nh} \int K\left( u \right) h du
= \frac{1}{n} \int K(u) du
= \frac{1}{n}
\end{equation*}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-21-1.pdf}

\hypertarget{propiedades-estaduxedsticas-1}{%
\section{Propiedades Estadísticas}\label{propiedades-estaduxedsticas-1}}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Podríamos imitar lo mismo que hicimos para el histograma?
\EndKnitrBlock{remark}

Si. Las propiedades que vimos anteriormente son universales para estimadores.

Entonces:
\begin{align*}
\mathrm{MSE}(\hat{f}_{h}(x)) & =\mathrm{Var}(\hat{f}_{h}(x))+\mathrm{Sesgo}^{2} (\hat{f}_{h}(x))            \\
\mathrm{MISE}(\hat{f}_{h})   & =\int\mathrm{Var}(\hat{f}_{h}(x))dx+\int\mathrm{Sesgo}^{2}(\hat{f}_{h}(x))dx
\end{align*}

donde

\(\mathrm{Var}\left(\hat{f}_{h}(x)\right)=\mathbb{E}\left[\hat{f}_{h}(x)-\mathbb{E}\hat{f}_{h}(x)\right]^{2}\) and \(\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)=\mathbb{E}\left[\hat{f}_{h}(x)\right]-f(x)\).

\hypertarget{varianza-1}{%
\subsection{Varianza}\label{varianza-1}}

\begin{align*}
\mathrm{Var}(\hat{f}_{h}(x))
& =\mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^{n}K\left(\frac{x-X_{i}}{h}\right)\right)          \\
& =\frac{1}{n^{2}h^{2}}\sum_{i=1}^{n}\mathrm{Var}\left(K\left(\frac{x-X_{i}}{h}\right)\right) \\
& =\frac{1}{nh^{2}}\mathrm{Var}\left(K\left(\frac{x-X}{h}\right)\right)                       \\
& =\frac{1}{nh^{2}}\left\{
\textcolor{red}{\mathbb{E}\left[K^{2}\left(\frac{x-X}{h}\right)\right]}
-\left\{
\textcolor{blue}{\mathbb{E}\left[K\left(\frac{x-X}{h}\right)\right]}
\right\}^{2}
\right\}.
\end{align*}
Usando que:
\begin{align*}
\textcolor{red}{\mathbb{E}\left[K^{2}\left(\frac{x-X}{h}\right)\right]}
& =\int K^{2}\left(\frac{x-s}{h}\right)f(s)ds            \\
& =h\int K^{2}\left(u\right)f(uh+x)du                    \\
& =h\int K^{2}\left(u\right)\left\{ f(x)+o(1)\right\} du \\
& =h\left\{ \Vert K\Vert_{2}^{2}f(x)+o(1)\right\} .
\end{align*}

\begin{align*}
\textcolor{blue}{\mathbb{E}\left[K\left(\frac{x-X}{h}\right)\right]}
& =\int K\left(\frac{x-s}{h}\right)f(s)ds            \\
& = h\int K\left(u\right)f(uh+x)du                    \\
& =h\int K\left(u\right)\left\{ f(x)+o(1)\right\} du \\
& =h\left\{f(x)+o(1)\right\} .
\end{align*}

Por lo tanto se obtiene que

\begin{equation*}
\mathrm{Var}\left(\hat{f}_{h}(x)\right) = \frac{1}{nh} \Vert K\Vert_{2}^{2}f(x) + o\left(\frac{1}{nh}\right), \text{ si } nh\to \infty.
\end{equation*}

\hypertarget{sesgo-1}{%
\subsection{Sesgo}\label{sesgo-1}}

Para el sesgo tenemos

\begin{align*}
\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)
& = \mathbb{E}\left[\hat{f}_{h}(x)\right]-f(x)                                                  \\
& = \frac{1}{nh} \sum_{i=1}^{n} \mathrm{E}\left[K\left( \frac{x-X_{i}}{h} \right)\right] - f(x) \\
& = \frac{1}{h}\mathrm{E}\left[K\left( \frac{x-X_{1}}{h} \right)\right] - f(x)                  \\
& = \int \frac{1}{h} K\left( \frac{x-u}{h}\right)f(u)du -f(x)                                   \\
\end{align*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-23}{}{\label{exr:unnamed-chunk-23} }Usando el cambio de variable \(s=\frac{u-x}{h}\) y las propiedades del kernel pruebe que

\begin{equation*}
\mathrm{Sesgo}\left(\hat{f}_{h}(x)\right) = \frac{h^{2}}{2} f^{\prime\prime} \mu_{2}(K) + o(h^{2}), \text{ si } h\to 0
\end{equation*}
donde \(\mu_{2}=\int s^{2}K(s)ds\).

\emph{**Nota:** En algunas pruebas más formales, se necesita
además que  $f^{\prime\prime}$ sea absolutamente continua y que
$\int(f^{\prime\prime\prime}(x))dx<\infty$.}
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solución. } \fi{}\begin{align*}
\mathrm{Sesgo}(\hat{f_{h}}(x)) & = \int \frac{1}{h} K\left( \frac{x-u}{h} \right) f(u)du - f(x)     \\
& = \frac{1}{h} \int hK(s)f(sh+x) ds - f(x) \\
& = \int K(s)\Biggl[ f(x) + f^{\prime}(x)(sh+x-x)  \\
&  \qquad  + \frac{f^{\prime\prime}(x)}{2}(sh+x-x)^2 + o(h^{2}) \Biggr] - f(x) \\
& = \int K(s)f(x)ds + \int hf^{\prime}(x)sK(s) ds  \\
& \qquad  + \int \frac{h^2}{2} f^{\prime\prime}(x)s^2K(s) ds + o(h^2) - f(x) \\
& = f(x) + 0 + \frac{h^2}{2}f^{\prime\prime}(x)\mu_{2}(K) + o(h^2) - f(x)   \\
& = \frac{h^2}{2}f^{\prime\prime}(x)\mu_{2}(K) + o(h^2) \\
\end{align*}
\EndKnitrBlock{solution}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-25-1.pdf}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Note como los cambios en el ancho de banda modifican la suavidad (sesgo) y el aplanamiento de la curva (varianza).
\EndKnitrBlock{remark}

\hypertarget{error-cuadruxe1tico-medio-y-error-cuadruxe1tico-medio-integrado}{%
\subsection{Error cuadrático medio y Error cuadrático medio integrado}\label{error-cuadruxe1tico-medio-y-error-cuadruxe1tico-medio-integrado}}

El error cuadrático medio se escribe
\begin{align*}
\mathrm{MSE}(\hat{f}_{h}(x))
& = \mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)^{2} + \mathrm{Var}\left(\hat{f}_{h}(x)\right)                                                 \\
& = \frac{h^{4}}{4}\left(\mu_{2}(K)f^{\prime\prime}(x)\right)^{2}+\frac{1}{nh}\Vert K\Vert_{2}^{2}f(x)+o(h^{4})+o\left(\frac{1}{nh}\right).
\end{align*}

Y el error cuadrático medio integrado se escribe como,
\begin{align*}
\mathrm{MISE}\left(\hat{f}_{h}\right) & = \int \mathrm{MSE}\left(\hat{f}_{h}(x)\right)dx                                                                                                        \\
& = \int \mathrm{Sesgo}\left(\hat{f}_{h}(x)\right)^{2} + \mathrm{Var}\left(\hat{f}_{h}(x)\right)dx                                                        \\
& = \frac{h^{4}}{4}\mu_{2}^{2}(K)\left\Vert f^{\prime\prime}(x)\right\Vert_{2}^{2} +\frac{1}{nh}\Vert K\Vert_{2}^{2}+o(h^{4})+o\left(\frac{1}{nh}\right).
\end{align*}

\hypertarget{ancho-de-banda-uxf3ptimo}{%
\subsection{Ancho de banda óptimo}\label{ancho-de-banda-uxf3ptimo}}

Minimizando el \(\mathrm{MISE}\) con respecto a \(h\) obtenemos
\begin{equation*}
h_{opt}=\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right).
\end{equation*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}De forma práctica, \(h_{opt}\) no es un estimador útil de \(h\) porque depende de \(\Vert f^{\prime\prime}\Vert_{2}^{2}\) que es desconocido.

Más adelante veremos otra forma de encontrar este estimador.
\EndKnitrBlock{remark}

Evaluando \(h_{opt}\) en el \(\mathrm{MISE}\) tenemos que

\begin{equation*}
\mathrm{MISE}(\hat{f}_{h})=\frac{5}{4}\left(\Vert K\Vert_{2}^{2}\right)^{4/5}\left(\Vert f^{\prime\prime}\Vert_{2}^{2}\mu_{2}(K)\right)^{2/5}n^{-4/5} = O\left( n^{-4/5} \right).
\end{equation*}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-28-1.pdf}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Formalmente, es posible probar que si \(f\) es \(\beta\) veces continuamente diferenciable y \(\int\left(f^{(\beta)}\right)^{2}<\infty\), entonces se tiene que
\[
{\displaystyle h_{opt}=O\left(n^{-\frac{1}{2\beta+1}}\right).}
\]
Por lo tanto se podría aproximar a una tasa paramétrica de convergencia si
\(\beta\) es grande.
\EndKnitrBlock{remark}

\hypertarget{escogiendo-el-ancho-de-banda}{%
\section{Escogiendo el ancho de banda}\label{escogiendo-el-ancho-de-banda}}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}La principal característica del ancho de banda
\begin{equation*}
h_{opt}=\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right).
\end{equation*}

ES QUE ¡NO FUNCIONA!
\EndKnitrBlock{remark}

Veremos dos métodos para determinar un \(h\) que funcione:

\begin{itemize}
\tightlist
\item
  Referencia normal.
\item
  Validación cruzada.
\end{itemize}

\hypertarget{referencia-normal}{%
\subsection{Referencia normal}\label{referencia-normal}}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Este método es más efectivo si se conoce que la verdadera distribución es bastante suave, unimodal y simétrica.

Más adelante veremos otro método para densidades más generales.
\EndKnitrBlock{remark}

Asuma que \(f\) es normal distribuida y se utiliza un kernel \(K\) gausiano. Entonces se tiene que

\begin{align*}
\hat{h}_{rn} & =\left(\frac{\Vert K\Vert_{2}^{2}}{\Vert f^{\prime\prime}\Vert_{2}^{2}\left(\mu_{2}(K)\right)^{2}n}\right)^{1/5}=O\left( n^{-1/5} \right) \\
& =1.06 \hat{\sigma} n^{-1/5}.
\end{align*}

donde

\begin{equation*}
\hat{\sigma} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} \left( x_{i}-\bar{x}^{2} \right)}
\end{equation*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-32}{}{\label{exr:unnamed-chunk-32} }Pruebe que la ecuación anterior es verdadera. Es decir, calcule \(\Vert K\Vert_{2}^{2}\), \(\Vert f^{\prime\prime}\Vert_{2}^{2}\) y \(\mu_{2}(K)\)
\EndKnitrBlock{exercise}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Un problema con \(\hat{h}_{rn}=1.06 \hat{\sigma} n^{-1/5}\) es su sensibilidad a los valores extremos.
\EndKnitrBlock{remark}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-34}{}{\label{exm:unnamed-chunk-34} }La varianza empírica de 1, 2, 3, 4, 5, es 2.5.

La varianza empírica de 1, 2, 3, 4, 5, 99, es 1538.
\EndKnitrBlock{example}

El rango intercuantil IQR se define como
\begin{equation*}
\mathrm{IQR}^{X} = Q^{X}_{3} - Q^{X}_{1}
\end{equation*}
donde \(Q^{X}_{1}\) y \(Q^{X}_{3}\) son el primer y tercer de un conjunto de datos \(X_{1},\ldots, X_n\).

Con el supuesto que \(X\sim \mathcal{N}(\mu,\sigma^{2})\) entonces \(\displaystyle Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)\).

Entonces,
\begin{align*}
\mathrm{IQR}
& = Q^{X}_{3} - Q^{X}_{1}                                                     \\
& = \left( \mu+\sigma Q^{Z}_{3} \right) - \left( \mu+\sigma Q^{Z}_{1} \right) \\
& = \sigma \left(Q^{Z}_{3} - Q^{Z}_{1} \right)                                \\
& \approx \sigma \left( 0.67 - (0.67) \right)                                 \\
& =1.34 \sigma.
\end{align*}

Por lo tanto \(\displaystyle \hat{\sigma} = \frac{\widehat{\mathrm{IQR}}^{X}}{1.34}\)

Podemos sustituir la varianza empírica de la fórmula inicial y tenemos
\begin{equation*}
\hat{h}_{rn} = 1.06 \frac{\widehat{\mathrm{IQR}}^{X}}{1.34} n^{-\frac{1}{5}} \approx 0.79\  \widehat{\mathrm{IQR}}^{X}\ n^{-\frac{1}{5}}
\end{equation*}

Combinando ambos estimadores, podemos obtener,

\begin{equation*}
\hat{h}_{rn} = 1.06 \min \left\{\frac{\widehat{\mathrm{IQR}}^{X}}{1.34}, \hat{\sigma }\right\} n^{-\frac{1}{5}}
\end{equation*}

\hypertarget{validaciuxf3n-cruzada}{%
\subsection{Validación Cruzada}\label{validaciuxf3n-cruzada}}

Defina el \emph{error cuadrático integrado} como
\begin{align*}
\mathrm{ISE}(\hat{f}_{h}) & =\int\left(\hat{f}_{h}(x)-f(x)\right)^{2}dx\nonumber                   \\
& =\int \hat{f}_{h}^{2}(x)dx-2\int \hat{f}_{h}(x)f(x)dx+\int f^{2}(x)dx.
\end{align*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}El MISE es el valor esperado del ISE.
\EndKnitrBlock{remark}

Nuestro objetivo es minimizar el ISE con respecto a \(h\).

Primero note que \(\int f^{2}(x)dx\) NO DEPENDE de \(h\). Podemos minimizar la expresión
\begin{equation*}
\mathrm{ISE}(\hat{f}_{h})-\int f^{2}(x)dx=
\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}
-2
\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}
\end{equation*}

Vamos a resolver esto en dos pasos partes

\textbf{Integral \(\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}\)}

El término \(\textcolor{blue}{\int\hat{f}_{h}(x)f(x)dx}\) es el valor esperado de
\(\mathrm{E}\left[\hat{f}(X)\right]\). Su estimador es
\begin{equation*}
\widehat{\mathrm{E}\left[\hat{f}(X)\right]}
= \frac{1}{n}\sum_{i=1}^{n}\hat{f}_{h}(X_{i})
=\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}
K\left(\frac{X_{j}-X_{i}}{h}\right).
\end{equation*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}El problema con esta expresión es que las observaciones que se usan para estimar la esperanza son las misma que se usan para estimar \(\hat{f}_{h}(x)\) (Se utilizan doble).
\EndKnitrBlock{remark}

La solución es remover la \(i^{\text{ésima}}\) observación de \(\hat{f}_{h}\) para cada \(i\).

Redefiniendo el estimador anterior tenemos \(\int \hat{f}_{h}(x)f(x)dx\) como
\[
\frac{1}{n}\sum_{i=1}^{n}\hat{f}_{h,-i}(X_{i}),
\]
donde
\[
\hat{f}_{h,-i}(x)=\frac{1}{(n-1)h}\sum_{\substack{j=1\\ j\neq i}}^{n}K\left( \frac{x-X_{i}}{h} \right) .
\]

\textbf{Integral \(\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}\)}

Siguiendo con el término \(\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}\) note que este se puede reescribir como

\begin{align*}
\textcolor{red}{\int\hat{f}_{h}^{2}(x)dx}
& =\int\left(\frac{1}{nh}\sum_{i=1}^{n}K\left( \frac{x-X_{i}}{h} \right)\right)^{2}dx                                    \\
& =\frac{1}{n^{2}h^{2}}\sum_{i=1}^{n}\sum_{i=1}^{n}\int K\left(\frac{x-X_{i}}{h}\right)K\left(\frac{x-X_{j}}{h}\right)dx \\
& =\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{i=1}^{n}\int K\left(u\right)K\left(\frac{X_{i}-X_{j}}{h}-u\right)du               \\
& =\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{i=1}^{n}K*K\left(\frac{X_{i}-X_{j}}{h}\right).
\end{align*}

donde \(K*K\) es la convolución de \(K\) consigo misma.

Finalmente tenemos la función,

\[
\mathrm{CV}(h)=\frac{1}{n^{2}h}\sum_{i=1}^{n}\sum_{j=1}^{n}K*K\left(\frac{X_{i}-X_{j}}{h}\right)-\frac{2}{(n-1)h}\sum_{i=1}^{n}\mathop{\sum_{j=1}^{n}}_{j\neq i}K\left( \frac{X_{i}-X_{j}}{h} \right).
\]

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Note que \(\mathrm{CV}(h)\) no depende de \(f\) o sus derivadas.
\EndKnitrBlock{remark}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Para efectos prácticos es mejor utilizar el criterio

\[
CV(h)=\int\hat{f}_{h}^{2}(x)dx-\frac{2}{(n-1)h}\sum_{i=1}^{n}\mathop{\sum_{j=1}^{n}}_{j\neq i}K\left( \frac{X_{i}-X_{j}}{h} \right)
\]
y luego calcular numéricamente la integral.
\EndKnitrBlock{remark}

\hypertarget{intervalos-de-confianza-para-estimadores-de-densidad-no-paramuxe9tricos}{%
\subsection{Intervalos de confianza para estimadores de densidad no paramétricos}\label{intervalos-de-confianza-para-estimadores-de-densidad-no-paramuxe9tricos}}

Usando los resultados anteriores y asumiendo que \(h=cn^{-\frac{1}{5}}\) entonces

\begin{equation*}
n^{-\frac{2}{5}} \left\{ \hat{f}_{h}(x) -f(x)\right\}
\xrightarrow{\mathcal{L}} \mathcal{N}\left(\underbrace{\frac{c^{2}}{2} f^{\prime\prime}
\mu_{2}(K)}_{b_{x}}, \underbrace{\frac{1}{c}f(x) \left\Vert K \right\Vert_{2}^{2}}_{v_{x}}\right).
\end{equation*}

Si \(z_{1-\frac{\alpha}{2}}\) es el cuantil \(1-\frac{\alpha}{2}\) de una distribución normal estándar, entonces

\begin{align*}
1-\alpha
& \approx \mathbb{P}\left(b_{x}-z_{1-\frac{\alpha}{2}} v_{x} \leq n^{2 / 5}\left\{\widehat{f}_{h}(x)-f(x)\right\} \leq b_{x}+z_{1-\frac{\alpha}{2}} v_{x}\right) \\
& =\mathbb{P}\left(\widehat{f}_{h}(x)-n^{-2 / 5}\left\{b_{x}+z_{1-\frac{\alpha}{2}} v_{x}\right\}\right.                                                         \\
& \qquad\qquad \left. \leq f(x)\leq \hat{f}_{h}(x)-n^{-2 / 5}\left\{b_{x}-z_{1-\frac{\alpha}{2}} v_{x}\right\}\right)
\end{align*}

Esta expresión nos dice que con una probabilidad de \(1-\alpha\) se tiene que

\begin{equation*}
\begin{aligned}
& \left[\hat{f}_{h}(x)-\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)-z_{1-\frac{\alpha}{2}} \sqrt{\frac{f(x)\|K\|_{2}^{2}}{n h}}\right. \\
& \left.\widehat{f}_{h}(x)-\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)+z_{1-\frac{a}{2}} \sqrt{\frac{f(x)\|K\|_{2}^{2}}{n h}}\right]
\end{aligned}
\end{equation*}

Al igual que en los casos anteriores, este invtervalo no es útil ya que depende de \(f(x)\) y \(f^{\prime\prime} (x)\).

Si \(h\) es pequeño relativamente a \(n^{-\frac{1}{5}}\) entonces el segundo término \(\frac{h^{2}}{2} f^{\prime \prime}(x) \mu_{2}(K)\) podría ser ignorado.

Podemos reemplazar \(f(x)\) por su estimador \(\hat{f}_{h}(x)\). Entonces tendríamos una intervalo aplicable a nuestro caso

\begin{equation*}
\left[\hat{f_{h}}(x)-z_{1-\frac{\alpha}{2}} \sqrt{\frac{\hat{f_{h}}(x)\|K\|_{2}^{2}}{n h}}, \hat{f}_{h}(x)+z_{1-\frac{\alpha}{2}} \sqrt{\frac{\hat{f}_{h}(x)\|\mathrm{K}\|_{2}^{2}}{n h}}\right]
\end{equation*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Este intervalo de confianza solo funciona en cada punto particular de \(f(x)\).

Existe una versión más general para determinar la banda de confianza de toda la función. Por favor revisar la página 62 de \autocite{Hardle2004}.
\EndKnitrBlock{remark}

\hypertarget{laboratorio}{%
\section{Laboratorio}\label{laboratorio}}

Comenzaremos con una librería bastante básica llamada \texttt{KernSmooth}.

\hypertarget{efecto-de-distintos-kernels-en-la-estimaciuxf3n}{%
\subsection{Efecto de distintos Kernels en la estimación}\label{efecto-de-distintos-kernels-en-la-estimaciuxf3n}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stockres.txt"}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.6118200 -0.0204085 -0.0010632 -0.0004988  0.0215999  0.1432286
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(KernSmooth)}

\NormalTok{fhat_normal <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"normal"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.05}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat_normal, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-42-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat_unif <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"box"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.05}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat_unif, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-42-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat_epanech <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epanech"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.05}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat_epanech, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-42-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat_biweight <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"biweight"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.05}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat_biweight, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-42-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat_triweight <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"triweight"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.05}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat_triweight, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-42-5.pdf}

\hypertarget{efecto-del-ancho-de-banda-en-la-estimaciuxf3n}{%
\subsection{Efecto del ancho de banda en la estimación}\label{efecto-del-ancho-de-banda-en-la-estimaciuxf3n}}

** Kernel uniforme **

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"box"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.001}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"box"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-43-2.pdf}

** Kernel Epanechnikov **

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.001}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-44-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fhat <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =} \FloatTok{0.5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fhat, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-44-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{suppressMessages}\NormalTok{(}\KeywordTok{library}\NormalTok{(tidyverse))}
\KeywordTok{library}\NormalTok{(gganimate)}

\NormalTok{fani <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{40}\NormalTok{)) \{}
\NormalTok{    f <-}\StringTok{ }\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =}\NormalTok{ b, }\DataTypeTok{gridsize =} \KeywordTok{length}\NormalTok{(x))}
\NormalTok{    fani <-}\StringTok{ }\NormalTok{fani }\OperatorTok{%>%}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(}\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{xreal =} \KeywordTok{sort}\NormalTok{(x), }
        \DataTypeTok{x =}\NormalTok{ f}\OperatorTok{$}\NormalTok{x, }\DataTypeTok{y =}\NormalTok{ f}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{bw =}\NormalTok{ b))}
\NormalTok{\}}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fani) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"Ancho de banda = \{closest_state\}"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{transition_states}\NormalTok{(bw) }\OperatorTok{+}\StringTok{ }\KeywordTok{view_follow}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}

\CommentTok{# anim_save('manual_figure/bandwidth-animation.gif')}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}
- Construya una variable llamada \texttt{u} que sea una secuencia de -0.15 a 0.15 con un paso de 0.01
- Asigne \texttt{x} a los datos \texttt{stockrel} y calcule su media y varianza.
- Usando la función \texttt{dnorm} construya los valores de la distribución de los datos usando la media y varianza calculada anteriormente. Asigne a esta variable \texttt{f\textbackslash{}\_param}.
- Defina un ancho de banda \texttt{h} en 0.02
- Construya un histograma para estos datos con ancho de banda \texttt{h}. Llame a esta variable \texttt{f\textbackslash{}\_hist}
- Usando el paquete \texttt{KernSmooth} y la función \texttt{bkde}, construya una función que calcule el estimador no paramétrico con un núcleo Epanechivok para un ancho de banda \(h\). Llame a esta variable \texttt{f\textbackslash{}\_epa}.
- Dibuje en el mismo gráfico la estimación paramétrica y no paramétrica.
\EndKnitrBlock{remark}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stockres.txt"}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(x)}
\CommentTok{# Eliminar nombres de las columnas}
\KeywordTok{names}\NormalTok{(x) <-}\StringTok{ }\OtherTok{NULL}

\NormalTok{u <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{)}

\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(x)}

\NormalTok{f_param <-}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(u, }\DataTypeTok{mean =}\NormalTok{ mu, }\DataTypeTok{sd =}\NormalTok{ sigma)}

\NormalTok{h <-}\StringTok{ }\FloatTok{0.02}

\NormalTok{n_bins <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\KeywordTok{diff}\NormalTok{(}\KeywordTok{range}\NormalTok{(x))}\OperatorTok{/}\NormalTok{h)}

\NormalTok{f_hist <-}\StringTok{ }\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{breaks =}\NormalTok{ n_bins)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-47-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f_epa <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =}\NormalTok{ h))}

\NormalTok{x_df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(x)}

\KeywordTok{library}\NormalTok{(ggplot2)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =} \FloatTok{0.02}\NormalTok{, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ f_epa, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-47-2.pdf}

\hypertarget{ancho-de-banda-uxf3ptimo-1}{%
\subsection{Ancho de banda óptimo}\label{ancho-de-banda-uxf3ptimo-1}}

Usemos la regla de la normal o también conocida como Silverman.
\textbf{Primero recuerde que en este caso se asume que \(f(x)\) sigue una distribución normal}. En este caso, lo que se obtiene es que

\begin{align*}
\Vert f^{\prime \prime} \Vert_2^2 & = \sigma ^{-5} \int \{\phi^{\prime \prime}\}^2 dx              \\
& = \sigma ^{-5} \frac{3}{8\sqrt{\pi}} \approx 0.212 \sigma^{-5}
\end{align*}

donde \(\phi\) es la densidad de una normal estándar.

El estimador para \(\sigma\) es

\[
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2  }.
\]

Y usando el cálculo realizado anteriormente, se obtiene que

\[
h_{normal} = \left( \frac{4 s^5}{3n} \right)^{1/5} \approx 1.06 s n^{-1/5}.
\]

Un estimador más robusto es

\[
h_{normal} =  1.06 \min \left\{ s , \frac{IQR}{1.34} \right\} n^{-1/5}.
\]

¿Por qué es \(IQR / 1.34\)?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(x)}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_normal <-}\StringTok{ }\FloatTok{1.06} \OperatorTok{*}\StringTok{ }\NormalTok{s }\OperatorTok{*}\StringTok{ }\NormalTok{n}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\OperatorTok{/}\DecValTok{5}\NormalTok{)}

\NormalTok{h <-}\StringTok{ }\NormalTok{h_normal}

\NormalTok{n_bins <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\KeywordTok{diff}\NormalTok{(}\KeywordTok{range}\NormalTok{(x))}\OperatorTok{/}\NormalTok{h)}
\NormalTok{f_hist <-}\StringTok{ }\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{breaks =}\NormalTok{ n_bins, }\DataTypeTok{plot =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{f_epa <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =}\NormalTok{ h))}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ f_epa, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-49-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_iqr <-}\StringTok{ }\FloatTok{1.06} \OperatorTok{*}\StringTok{ }\KeywordTok{min}\NormalTok{(s, }\KeywordTok{IQR}\NormalTok{(x)}\OperatorTok{/}\FloatTok{1.34}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{n}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\OperatorTok{/}\DecValTok{5}\NormalTok{)}

\NormalTok{h <-}\StringTok{ }\NormalTok{h_iqr}

\NormalTok{n_bins <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\KeywordTok{diff}\NormalTok{(}\KeywordTok{range}\NormalTok{(x))}\OperatorTok{/}\NormalTok{h)}
\NormalTok{f_hist <-}\StringTok{ }\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{breaks =}\NormalTok{ n_bins, }\DataTypeTok{plot =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{f_epa <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{bkde}\NormalTok{(x, }\DataTypeTok{kernel =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{bandwidth =}\NormalTok{ h))}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ f_epa, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-50-1.pdf}

Una librería más especializada es \texttt{np} (non-parametric).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(np)}

\NormalTok{x.eval <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{200}\NormalTok{)}

\NormalTok{h_normal_np <-}\StringTok{ }\KeywordTok{npudensbw}\NormalTok{(}\DataTypeTok{dat =}\NormalTok{ x, }\DataTypeTok{bwmethod =} \StringTok{"normal-reference"}\NormalTok{)}

\NormalTok{dens.ksum <-}\StringTok{ }\KeywordTok{npksum}\NormalTok{(}\DataTypeTok{txdat =}\NormalTok{ x, }\DataTypeTok{exdat =}\NormalTok{ x.eval, }\DataTypeTok{bws =}\NormalTok{ h_normal_np}\OperatorTok{$}\NormalTok{bw)}\OperatorTok{$}\NormalTok{ksum}\OperatorTok{/}\NormalTok{(n }\OperatorTok{*}\StringTok{ }
\StringTok{    }\NormalTok{h_normal_np}\OperatorTok{$}\NormalTok{bw[}\DecValTok{1}\NormalTok{])}

\NormalTok{dens.ksum.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x.eval, }\DataTypeTok{y =}\NormalTok{ dens.ksum)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h_normal_np}\OperatorTok{$}\NormalTok{bw, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dens.ksum.df, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-51-1.pdf}

\hypertarget{validaciuxf3n-cruzada-1}{%
\subsection{Validación cruzada}\label{validaciuxf3n-cruzada-1}}

La forma que vimos en clase es la de validación cruzada por mínimos
cuadrados``least-square cross validation'' la cual se puede ejecutar
con este comando.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_cv_np_ls <-}\StringTok{ }\KeywordTok{npudensbw}\NormalTok{(}\DataTypeTok{dat =}\NormalTok{ x, }\DataTypeTok{bwmethod =} \StringTok{"cv.ls"}\NormalTok{, }
    \DataTypeTok{ckertype =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{ckerorder =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 /Multistart 1 of 1 |Multistart 1 of 1 |                   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np <-}\StringTok{ }\KeywordTok{npudens}\NormalTok{(h_cv_np_ls)}

\KeywordTok{plot}\NormalTok{(dens.np, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{eval[, }\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{dens)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h_cv_np_ls}\OperatorTok{$}\NormalTok{bw, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dens.np.df, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-53-1.pdf}

\hypertarget{temas-adicionales}{%
\subsection{Temas adicionales}\label{temas-adicionales}}

** Reducción del sesgo **
Como lo mencionamos en el texto, una forma de mejorar el sesgo en la estimación es suponer que la función de densidad es más veces diferenciable.

Esto se logra asumiendo que el Kernel es más veces diferenciable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_cv_np_ls <-}\StringTok{ }\KeywordTok{npudensbw}\NormalTok{(}\DataTypeTok{dat =}\NormalTok{ x, }\DataTypeTok{bwmethod =} \StringTok{"cv.ls"}\NormalTok{, }
    \DataTypeTok{ckertype =} \StringTok{"epa"}\NormalTok{, }\DataTypeTok{ckerorder =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 /Multistart 1 of 1 |Multistart 1 of 1 |                   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np <-}\StringTok{ }\KeywordTok{npudens}\NormalTok{(h_cv_np_ls)}

\KeywordTok{plot}\NormalTok{(dens.np, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-54-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{eval[, }\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{dens)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h_cv_np_ls}\OperatorTok{$}\NormalTok{bw, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dens.np.df, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-55-1.pdf}

\textbf{Otra forma de estimar el ancho de banda} Otra forma de estimar ancho de bandas óptimos es usando máxima verosimilitud. Les dejo de tarea revisar la sección 1.1 del artículo de \autocite{Hall1987} para entender su estructura.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_cv_np_ml <-}\StringTok{ }\KeywordTok{npudensbw}\NormalTok{(}\DataTypeTok{dat =}\NormalTok{ x, }\DataTypeTok{bwmethod =} \StringTok{"cv.ml"}\NormalTok{, }
    \DataTypeTok{ckertype =} \StringTok{"epanechnikov"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 /Multistart 1 of 1 |Multistart 1 of 1 |                   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np <-}\StringTok{ }\KeywordTok{npudens}\NormalTok{(h_cv_np_ml)}

\KeywordTok{plot}\NormalTok{(dens.np, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-56-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{eval[, }\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{dens)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h_cv_np_ml}\OperatorTok{$}\NormalTok{bw, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dens.np.df, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-57-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_cv_np_ml <-}\StringTok{ }\KeywordTok{npudensbw}\NormalTok{(}\DataTypeTok{dat =}\NormalTok{ x, }\DataTypeTok{bwmethod =} \StringTok{"cv.ml"}\NormalTok{, }
    \DataTypeTok{ckertype =} \StringTok{"epanechnikov"}\NormalTok{, }\DataTypeTok{ckerorder =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 |Multistart 1 of 1 /Multistart 1 of 1 |Multistart 1 of 1 |                   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np <-}\StringTok{ }\KeywordTok{npudens}\NormalTok{(h_cv_np_ml)}

\KeywordTok{plot}\NormalTok{(dens.np, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-58-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dens.np.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{eval[, }\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ dens.np}\OperatorTok{$}\NormalTok{dens)}

\KeywordTok{ggplot}\NormalTok{(x_df, }\KeywordTok{aes}\NormalTok{(x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{binwidth =}\NormalTok{ h_cv_np_ml}\OperatorTok{$}\NormalTok{bw, }\DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mu, }
        \DataTypeTok{sd =}\NormalTok{ sigma), }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dens.np.df, }
    \KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-59-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fani <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{40}\NormalTok{)) \{}
\NormalTok{    f <-}\StringTok{ }\KeywordTok{npudens}\NormalTok{(}\DataTypeTok{tdat =}\NormalTok{ x, }\DataTypeTok{ckertype =} \StringTok{"epanechnikov"}\NormalTok{, }
        \DataTypeTok{bandwidth.compute =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{bws =}\NormalTok{ b)}
\NormalTok{    fani <-}\StringTok{ }\NormalTok{fani }\OperatorTok{%>%}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(}\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{xreal =} \KeywordTok{sort}\NormalTok{(x), }
        \DataTypeTok{x =}\NormalTok{ f}\OperatorTok{$}\NormalTok{eval}\OperatorTok{$}\NormalTok{x, }\DataTypeTok{y =}\NormalTok{ f}\OperatorTok{$}\NormalTok{dens, }\DataTypeTok{bw =}\NormalTok{ b))}
\NormalTok{\}}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fani) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, y), }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"Ancho de banda = \{closest_state\}"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{20}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{transition_states}\NormalTok{(bw) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{view_follow}\NormalTok{()}

\CommentTok{# anim_save('manual_figure/bandwidth-animation-np.gif')}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-61}{}{\label{exr:unnamed-chunk-61} }Implementar el intervalo confianza visto en clase para estimadores de densidades por núcleos y visualizarlo de en ggplot.

\textbf{Si se atreven: ¿Se podría hacer una versión animada de ese gráfico para visualizar el significado real de este el intervalo de confianza?}
\EndKnitrBlock{exercise}

\hypertarget{jacknife-y-bootstrap}{%
\chapter{Jacknife y Bootstrap}\label{jacknife-y-bootstrap}}

Suponga que se quiere estimar un intervalo de confianza para la media
\(\mu\) desconocida de un conjunto de datos \(X_{1},\ldots, X_{n}\)
que tiene distribución \(\mathcal{N}\left(\mu ,\sigma^{2}\right)\).

Primero se conoce que

\begin{equation*}
\sqrt{n}\left( \hat{\mu} - \mu \right)
\xrightarrow{\mathcal{L}} \mathcal{N}\left(0,\sigma^{2}\right),
\end{equation*}

y esto nos permite escribir el intervalo de confianza como

\begin{equation*}
\left[ \hat{\mu} - \hat{\sigma}z_{1-\frac{\alpha}{2}} ,
\hat{\mu} + \hat{\sigma}z_{1-\frac{\alpha}{2}}\right]
\end{equation*}

donde \(z_{1-\frac{\alpha}{2}}\) es el cuantil \(1-\frac{\alpha}{2}\)
de una normal estándar.

La expresión anterior es posible ya que el supuesto es que la
distribución de \(\hat{\theta}\) es normal.

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Qué pasaría si este supuesto es falso o al menos no conocemos la
distribución de \(\hat{\theta}\)?

¿Cómo podemos encontrar ese intervalo de confianza?
\EndKnitrBlock{remark}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Para una muestra fija, el estimador anterior \(\hat{\mu}\)
solamente un valor. No se conoce la distribución de \(\hat{\mu}\). Lo
único que se puede estimar son valores puntuales como la media,
varianza, mediana, etc, pero no sabemos nada de su distribución.
\EndKnitrBlock{remark}

\hypertarget{caso-concreto}{%
\section{Caso concreto}\label{caso-concreto}}

Suponga que tenemos la siguiente tabla de datos, que representa una
muestra de tiempos y distancias de viajes en Atlanta.

Cargamos la base de la siguiente forma:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CommuteAtlanta <-}\StringTok{ }\KeywordTok{read.csv2}\NormalTok{(}\StringTok{"data/CommuteAtlanta.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|l}
\hline
City & Age & Distance & Time & Sex\\
\hline
Atlanta & 19 & 10 & 15 & M\\
\hline
Atlanta & 55 & 45 & 60 & M\\
\hline
Atlanta & 48 & 12 & 45 & M\\
\hline
Atlanta & 45 & 4 & 10 & F\\
\hline
Atlanta & 48 & 15 & 30 & F\\
\hline
Atlanta & 43 & 33 & 60 & M\\
\hline
\end{tabular}

Para este ejemplo tomaremos la variable \texttt{Time} que la
llamaremos \texttt{x} para ser más breves. En este caso note que

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{CommuteAtlanta}\OperatorTok{$}\NormalTok{Time}
\end{Highlighting}
\end{Shaded}

La media es 29.11 y su varianza 429.2483968. Para efectos de lo que sigue, asignaremos la varianza a la variable \(T_n\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tn <-}\StringTok{ }\KeywordTok{var}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

A partir de estos dos valores, ¿Cuál sería un intervalo de confianza
para la media?

Note que esta pregunta es difícil ya que no tenemos ningún tipo de
información adicional.

Las dos técnicas que veremos a continuación nos permitirán extraer
\emph{información adicional} de la muestra.

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Para efectos de este capítulo, llamaremos \(T_{n}=T\left(  X_{1},\ldots,X_{n}\right)\) al estadístico formado por la muestra de
los \(X_{i}\)'s.
\EndKnitrBlock{remark}

\hypertarget{jacknife}{%
\section{Jacknife}\label{jacknife}}

Esta técnica fue propuesta por \cite{Quenouille1949} y consiste en la
siguiente observación.

Se puede probar que muchos de los estimadores tiene la propiedad que

\begin{equation}
\operatorname{Sesgo}\left(T_{n}\right)=\frac{a}{n}+\frac{b}{n^{2}}+O\left(\frac{1}{n^{3}}\right)
\end{equation}

para algún \(a\) and \(b\).

Por ejemplo \(\sigma^{2}=\mathrm{Var}\left(X_{i}\right)\) y sea
\(\widehat{\sigma}_{n}^{2}=n^{-1} \sum_{i=1}^{n}\left(X_{i}-\right.\)
\(\bar{X})^{2}\). Entonces,

\begin{equation*}
\mathbb{E}\left(\widehat{\sigma}_{n}^{2}\right)=
\frac{n-1}{n}\sigma^{2}
\end{equation*}

por lo tanto

\begin{equation*}
\mathrm{Sesgo} = -\frac{\sigma^{2}}{n}
\end{equation*}

Por lo tanto en este caso \(a=-\sigma^{2}\) y \(b=0\).

Defina \(T_{(-i)}\) como el estimador \(T_{n}\) pero eliminando el
\(i\)-ésimo término.

Es claro que en este contexto, se tiene que

\begin{equation}
\operatorname{Sesgo}\left(T_{(-i)}\right)=\frac{a}{n-1}+\frac{b}{(n-1)^{2}}+O\left(\frac{1}{(n-1)^{3}}\right)
\end{equation}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-67}{}{\label{exr:unnamed-chunk-67} }Una forma fácil de construir los \(T_{(-i)}\) es primero replicando
la matriz de datos múltiple veces usando el producto de kronecker
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\NormalTok{jackdf <-}\StringTok{ }\KeywordTok{kronecker}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, n), x)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r}
\hline
15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15\\
\hline
60 & 60 & 60 & 60 & 60 & 60 & 60 & 60 & 60 & 60\\
\hline
45 & 45 & 45 & 45 & 45 & 45 & 45 & 45 & 45 & 45\\
\hline
10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10\\
\hline
30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30\\
\hline
60 & 60 & 60 & 60 & 60 & 60 & 60 & 60 & 60 & 60\\
\hline
45 & 45 & 45 & 45 & 45 & 45 & 45 & 45 & 45 & 45\\
\hline
10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10\\
\hline
25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25\\
\hline
15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15\\
\hline
\end{tabular}

Y luego se elimina la diagonal

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diag}\NormalTok{(jackdf) <-}\StringTok{ }\OtherTok{NA}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r}
\hline
NA & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15\\
\hline
60 & NA & 60 & 60 & 60 & 60 & 60 & 60 & 60 & 60\\
\hline
45 & 45 & NA & 45 & 45 & 45 & 45 & 45 & 45 & 45\\
\hline
10 & 10 & 10 & NA & 10 & 10 & 10 & 10 & 10 & 10\\
\hline
30 & 30 & 30 & 30 & NA & 30 & 30 & 30 & 30 & 30\\
\hline
60 & 60 & 60 & 60 & 60 & NA & 60 & 60 & 60 & 60\\
\hline
45 & 45 & 45 & 45 & 45 & 45 & NA & 45 & 45 & 45\\
\hline
10 & 10 & 10 & 10 & 10 & 10 & 10 & NA & 10 & 10\\
\hline
25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & NA & 25\\
\hline
15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & 15 & NA\\
\hline
\end{tabular}

Cada columna contiene toda la muestra excepto el \(i\)-ésimo
elemento. Solo basta estimar la media de cada columna:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T_i <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(jackdf, }\DecValTok{2}\NormalTok{, var, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r}
\hline
x\\
\hline
429.7098\\
\hline
428.1905\\
\hline
429.6023\\
\hline
429.3756\\
\hline
430.1087\\
\hline
428.1905\\
\hline
429.6023\\
\hline
429.3756\\
\hline
430.0764\\
\hline
429.7098\\
\hline
\end{tabular}

Definamos el sesgo \emph{jackife} como

\begin{equation*}
b_{jack} = (n-1) (\overline{T}_{n} - T_{n})
\end{equation*}

donde
\begin{equation*}
\overline{T}_{n} = \frac{1}{n} \sum_{i=1}^{n} T_{(-i)}
\end{equation*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-71}{}{\label{exr:unnamed-chunk-71} }En nuestro caso tendríamos lo siguiente:
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(bjack <-}\StringTok{ }\NormalTok{(n }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{mean}\NormalTok{(T_i) }\OperatorTok{-}\StringTok{ }\NormalTok{Tn))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

Es decir, que los \texttt{T\_i} generan estimadores de \texttt{T\_n}
que contienen el mismo sesgo.

Observe que \(b_{jack}\) tiene la siguiente propiedad

\begin{align*}
\mathbb{E}\left(b_{\text {jack}}\right)
&= (n-1)\left(\mathbb{E}\left[\overline{T}_{n}\right] -
\mathbb{E}\left[T_{n}\right]\right) \\
&= (n-1)\left(\mathbb{E}\left[\overline{T}_{n}\right] - \theta +
\theta - \mathbb{E}\left[T_{n}\right]\right) \\
& =(n-1)\left(\mathrm{Sesgo} \left(\overline{T}_{n}\right)
-\mathrm{Sesgo}\left(T_{n}\right)\right) \\
& =(n-1)\left[\left(\frac{1}{n-1}
-\frac{1}{n}\right)
a+\left(\frac{1}{(n-1)^{2}}
-\frac{1}{n^{2}}\right) b+O\left(\frac{1}{n^{3}}\right)\right] \\
& =\frac{a}{n}
+\frac{(2 n-1) b}{n^{2}(n-1)}
+O\left(\frac{1}{n^{2}}\right) \\
& =\operatorname{Sesgo}\left(T_{n}\right)
+O\left(\frac{1}{n^{2}}\right)\\
\end{align*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Es decir, en general, el estimador \(b_{\text{jack}}\) aproxima
correctamente \(\mathrm{Sesgo}\left( T_{n} \right)\) hasta con un
error del \(n^{-2}\).
\EndKnitrBlock{remark}

Podemos usar los \(T\_i\) para generar muestras adicionales para
estimar el parámetro \(\theta\).

En este caso defina el siguiente estimador:

\[
\widetilde{T}_{i}=n T_{n}-(n-1) T_{(-i)}.
\]

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}A \(\widetilde{T}_{i}\) se le llaman \textbf{pseudo-valores} y
representa el aporte o peso que tiene la variable \(X_{i}\) para
estimar \(T_{n}\).
\EndKnitrBlock{remark}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-75}{}{\label{exr:unnamed-chunk-75} }Usado un cálculo similar para el \(b_{jack}\) pruebe que

\[
\operatorname{Sesgo}\left(T_{\text {jack}
}\right)=-\frac{b}{n(n-1)}+O\left(\frac{1}{n^{2}}\right)=O\left(\frac{1}{n^{2}}\right).
\]

¿Qué conclusión se obtiene de este cálculo?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-76}{}{\label{exr:unnamed-chunk-76} }Los pseudo-valores se estiman de forma directa como,
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pseudo <-}\StringTok{ }\NormalTok{n }\OperatorTok{*}\StringTok{ }\NormalTok{Tn }\OperatorTok{-}\StringTok{ }\NormalTok{(n }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{T_i}

\NormalTok{pseudo[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 199.02972209 957.16225222 252.64417993 365.79679037  -0.06666345
##  [6] 957.16225222 252.64417993 365.79679037  16.09799519 199.02972209
\end{verbatim}

Lo importante acá es notar la similitud que tiene con los datos
reales,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ pseudo)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-78-1.pdf}

Con estos pseudo-valores, es posible estimar la media y la varianza de
\(T_{n}\) con sus respectivos estimadores:

\[
T_{\text {jack }}=\frac{1}{n} \sum_{i=1}^{n} \widetilde{T}_{i}
\]

donde

\[
v_{jack}=\frac{\sum_{i=1}^{n}\left(\widetilde{T}_{i}-\frac{1}{n}
\sum_{i=1}^{n} \widetilde{T}_{i}\right)^{2}}{n(n-1)}.
\]

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}
Sin embargo, se puede demostrar fácilmente que se pueden usar
pseudovalores para construir una prueba normal de hipótesis. Dado que
cada pseudovalor es independiente e idénticamente distribuido (iid),
se deduce que su promedio se ajusta a una distribución normal a medida
que el tamaño de la muestra aumenta. El promedio de los pseudovalores
es solo \(T_ {jack}\) y el valor esperado de ese promedio, debido a la
construcción a la imparcialidad del estimador, es el parámetro bajo
investigación, \(\theta\). Por lo tanto, tenemos que
\[
  \frac{\sqrt{n}\left(T_{jack}-\theta\right)}{\sqrt{v_{jack}}}
  \rightarrow N(0,1).
\]
\EndKnitrBlock{remark}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-80}{}{\label{exr:unnamed-chunk-80} }
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Tjack <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(pseudo))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 429.2484
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Vjack <-}\StringTok{ }\KeywordTok{var}\NormalTok{(pseudo, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2701991
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(sdjack <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(Vjack))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1643.774
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(z <-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.959964
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(Tjack }\OperatorTok{-}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdjack}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n), Tjack }\OperatorTok{+}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdjack}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 285.1679 573.3289
\end{verbatim}

\hypertarget{bootstrap}{%
\section{Bootstrap}\label{bootstrap}}

Este método es un poco más sencillo de implementar que Jacknife y es
igualmente de eficaz propuesto por \cite{Efron1979}.

Primero recordemos que estamos estimando una estadístico a partir de
una muestra de modo que \(T_{n}=g\left( X_{1},\ldots,X_{n} \right)\)
donde \(g\) es cualquier función (media, varianza, quantiles, etc).

Supongamos que conocemos la distribución real de los \(X\)'s, llamada \(F(x)\). Si uno
quisiera estimar la varianza de \(X\) basta con hacer

\begin{equation*}
\mathrm{Var}_{F}\left(T_{n}\right)
= \frac{\sigma^{2}}{n}=\frac{\int x^{2}  dF(x)-\left(\int x
dF(x)\right)^{2}}{n}
\end{equation*}

donde \(\sigma^{2} = \mathrm{Var}\left(X\right)\) y el subindice \(F\) es solo para indicar la dependencia con la distribución real.

Ahora dado que no tenemos la distribución real \(F(x)\), una opción es encontrar un estimador de esta llamado \(\hat{F}_n\).

La técnica de boostrap se basa en extraer muchas muestras iid de la distribución \(\hat{F}_n\) de modo que se pueda conocer su varianza.

En simple pasos la técnica es

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Seleccione \(X_{1}^{*}, \ldots, X_{n}^{*} \sim \widehat{F}_{n}\)
\item
  Estime \(T_{n}^{*}=g\left(X_{1}^{*}, \ldots, X_{n}^{*}\right)\)
\item
  Repita los Pasos 1 y 2, \(B\) veces para obtener \(T_{n, 1}^{*}, \ldots, T_{n, B}^{*}\)
\item
  Estime
  \[
  v_{\mathrm{boot}}=\frac{1}{B} \sum_{b=1}^{B}\left(T_{n, b}^{*}-\frac{1}{B} \sum_{r=1}^{B} T_{n, r}^{*}\right)^{2}
  \]
\end{enumerate}

Por la ley de los grandes números tenemos que

\begin{equation}
v_{\mathrm{boot}} \stackrel{\mathrm{a.s.}}{\longrightarrow} \mathbb{V}_{\widehat{F}_{n}}\left(T_{n}\right), \text {\quad si } B \rightarrow \infty.
\end{equation}

además llamaremos,

\begin{equation*}
\widehat{\mathrm{se}}_{\mathrm{boot}}=\sqrt{v_{\mathrm{boot}}}
\end{equation*}

En pocas palabras lo que tenemos es que

\begin{align*}
\text  {Mundo Real: }
& F
& \Longrightarrow  X_{1}, \ldots, X_{n}
& \Longrightarrow
& T_{n} = g\left(X_{1}, \ldots, X_{n}\right) \\
\text {Mundo Bootstrap: }
& \widehat{F}_{n}
& \Longrightarrow  X_{1}^{*}, \ldots, X_{n}^{*}
& \Longrightarrow
& T_{n}^{*}=g\left(X_{1}^{*}, \ldots, X_{n}^{*}\right)
\end{align*}

En términos de convergencia lo que se tiene es que
\[
\mathrm{Var}_{F}\left(T_{n}\right) \overbrace{\approx}^{O(1 / \sqrt{n})} \mathrm{Var}_{\widehat{F}_{n}}\left(T_{n}\right) \overbrace{\approx}^{O(1 / \sqrt{B})} v_{b o o t}
\]

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}¿Cómo extraemos una muestra de \(\hat{F}_n\)?
\EndKnitrBlock{remark}

Recuerden que \(\hat{F}_{n}\) asigna la probabilidad de \(\frac{1}{n}\) a cada valor usado para construirla.

Por lo tanto, todos los puntos originales \(X_{1},\ldots,X_{n}\) tienen probabilidad \(\frac{1}{n}\) de ser escogidos, que resulta ser equivalente a un muestreo con remplazo \(n\)-veces.

Así que basta cambiar el punto 1. del algoritmo mencionando anteriormente con

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Seleccione una muestra con remplazo \(X_{1}^{*}, \ldots, X_{n}^{*}\) de \(X_{1},\ldots,X_{n}\).
\end{enumerate}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-87}{}{\label{exr:unnamed-chunk-87} }En este ejemplo podemos tomar \(B=1000\) y construir esa cantidad de veces nuestro estimador.
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\DecValTok{1000}
\NormalTok{Tboot_b <-}\StringTok{ }\OtherTok{NULL}

\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{B) \{}
\NormalTok{    xb <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{size =}\NormalTok{ n, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    Tboot_b[b] <-}\StringTok{ }\KeywordTok{var}\NormalTok{(xb)}
\NormalTok{\}}

\NormalTok{Tboot_b[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 345.1819 493.5279 273.3998 446.3071 426.0340 384.2662 383.2132 455.8139
##  [9] 462.3363 594.5774
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Tboot_b)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-89-1.pdf}

Por supuesto podemos encontrar los estadísticos usuales para esta nueva muestra

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Tboot <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Tboot_b))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 428.066
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(Vboot <-}\StringTok{ }\KeywordTok{var}\NormalTok{(Tboot_b))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5504.701
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(sdboot <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(Vboot))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 74.19367
\end{verbatim}

\texttt{}

\hypertarget{intervalos-de-confianza}{%
\subsection{Intervalos de confianza}\label{intervalos-de-confianza}}

\subsubsection{Intervalo Normal}

Este es el más sencillo y se escribe como

\begin{equation}
T_{n} \pm z_{\alpha / 2} \widehat{\mathrm{Se}}_{\mathrm{boot}}
\end{equation}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Este intervalo solo funciona si la distribución de \(T_{n}\) es normal.
\EndKnitrBlock{remark}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-94}{}{\label{exr:unnamed-chunk-94} }El cálculo de este intervalo es
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(Tn }\OperatorTok{-}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot, Tn }\OperatorTok{+}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 283.8315 574.6653
\end{verbatim}

\texttt{}

\subsubsection{Intervalo pivotal}

Sea \(\theta=T(F)\) y \(\widehat{\theta}_{n}=T\left(\widehat{F}_{n}\right)\) y defina la cantidad pivotal \(R_{n}=\widehat{\theta}_{n}-\theta .\)

Sea \(H(r)\) la función de distribución del pivote:
\[
H(r)=\mathbb{P}_{F}\left(R_{n} \leq r\right).
\]

Además considere \(C_{n}^{\star}=(a, b)\) donde
\[
a=\widehat{\theta}_{n}-H^{-1}\left(1-\frac{\alpha}{2}\right) \quad \text { y } \quad b=\widehat{\theta}_{n}-H^{-1}\left(\frac{\alpha}{2}\right).
\]

Se sigue que
\begin{align*}
\mathbb{P}(a \leq \theta \leq b)
&=\mathbb{P}\left(\widehat{\theta}_{n}-b \leq R_{n} \leq \widehat{\theta}_{n}-a\right) \\
&=H\left(\widehat{\theta}_{n}-a\right)-H\left(\widehat{\theta}_{n}-b\right) \\
&=H\left(H^{-1}\left(1-\frac{\alpha}{2}\right)\right)-H\left(H^{-1}\left(\frac{\alpha}{2}\right)\right) \\
&=1-\frac{\alpha}{2}-\frac{\alpha}{2}=1-\alpha
\end{align*}
\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}\(C_{n}^{\star}=(a, b)\) es un intervalo de confianza al \(1-\alpha\) de confianza.

El problema es que este intervalo depende de \(H\) desconocido.
\EndKnitrBlock{remark}

Para resolver este problema, se puede construir una versión \emph{bootstrap} de \(H\) usando lo que sabemos hasta ahora.

\[
\widehat{H}(r)=\frac{1}{B} \sum_{b=1}^{B} I\left(R_{n, b}^{*} \leq r\right)
\]
donde \(R_{n, b}^{*}=\widehat{\theta}_{n, b}^{*}-\widehat{\theta}_{n}\).

Sea \(r_{\beta}^{*}\) el cuantil muestral de tamaño \(\beta\) de \(\left(R_{n, 1}^{*}, \ldots, R_{n, B}^{*}\right)\) y sea \(\theta_{\beta}^{*}\) el cuantil muestral de tamaño \(\beta\) de \(\left(\theta_{n, 1}^{*}, \ldots, \theta_{n, B}^{*}\right)\).

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Según la notación anterior note que
\begin{equation*}
r_{\beta}^{*}= \theta_{\beta}^{*}-\widehat{\theta}_{n}
\end{equation*}
\EndKnitrBlock{remark}

Con estas observaciones
It follows that an approximate \(1-\alpha\) confidence interval is \(C_{n}=(\widehat{a}, \widehat{b})\) where

\begin{align*}
\widehat{a}
&= \widehat{\theta}_{n}-\widehat{H}^{-1}\left(1-\frac{\alpha}{2}\right)
&= \widehat{\theta}_{n}-r_{1-\alpha / 2}^{*}
&= \widehat{\theta}_{n}-\theta_{1-\alpha / 2}^{*} + \widehat{\theta}_{n}
&=2 \widehat{\theta}_{n}-\theta_{1-\alpha / 2}^{*} \\
\widehat{b} &=\widehat{\theta}_{n}-\widehat{H}^{-1}\left(\frac{\alpha}{2}\right)
&=\widehat{\theta}_{n}-r_{\alpha / 2}^{*}
&= \widehat{\theta}_{n}-\theta_{\alpha / 2}^{*} + \widehat{\theta}_{n}
&=2 \widehat{\theta}_{n}-\theta_{\alpha / 2}^{*}
\end{align*}

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}El intervalo de confianza pivotal de tamaño \(1-\alpha\) es
\[
  C_{n}=\left(2 \widehat{\theta}_{n}-\widehat{\theta}_{((1-\alpha / 2) B)}^{*}, 2 \widehat{\theta}_{n}-\widehat{\theta}_{((\alpha / 2) B)}^{*}\right)
  \]
\EndKnitrBlock{remark}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-99}{}{\label{exr:unnamed-chunk-99} }El intervalo anterior para un nivel de 95\% se estima de la siguiente forma
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(Tboot_b, }\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{), }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{Tn }\OperatorTok{-}\StringTok{ }
\StringTok{    }\KeywordTok{quantile}\NormalTok{(Tboot_b, }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    97.5%     2.5% 
## 267.1250 552.9294
\end{verbatim}

\texttt{}

\hypertarget{intervalo-pivotal-studentizado}{%
\subsection{Intervalo pivotal studentizado}\label{intervalo-pivotal-studentizado}}

Una mejora del intervalo anterior sería normalizar los estimadores previamente

\[
Z_{n}=\frac{T_{n}-\theta}{\widehat{\mathrm{se}}_{\mathrm{boot}}}.
\]
Como \(\theta\) es desconocido, entonces la versión a estimar es
\[
Z_{n, b}^{*}=\frac{T_{n, b}^{*}-T_{n}}{\widehat{\mathrm{se}}_{b}^{*}}
\]
donde \(\widehat{\mathrm{se}}_{b}^{*}\) es un estimador del error estándar de \(T_{n, b}^{*}\) no de \(T_{n}\).

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}Esto requerira estimar la varianza de \(T_{n,b}^*\) para cada \(b\).
\EndKnitrBlock{remark}

Con esto se puede obtener cantidades \(Z_{n, 1}^{*}, \ldots, Z_{n, B}^{*}\) que debería ser próximos a \(Z_{n}\).

Sea \(z_{\alpha}^{*}\) del \(\alpha\) cuantiĺ de \(Z_{n, 1}^{*}, \ldots, Z_{n, B}^{*},\) entonces \(\mathbb{P}\left(Z_{n} \leq z_{\alpha}^{*}\right) \approx \alpha\).

Define el intervalo
\begin{equation*}
C_{n}=\left(T_{n}-z_{1-\alpha / 2}^{*} \widehat{\mathrm{se}}_{\mathrm{boot}}, T_{n}-z_{\alpha / 2}^{*} \widehat{\mathrm{se}}_{\mathrm{boot}}\right)
\end{equation*}

Justificado por el siguiente cálculo:

\begin{align*}
\mathbb{P}\left(\theta \in C_{n}\right) &=\mathbb{P}\left(T_{n}-z_{1-\alpha / 2}^{*} \widehat{\mathrm{Se}}_{\mathrm{boot}} \leq \theta \leq T_{n}-z_{\alpha / 2}^{*} \widehat{\mathrm{Se}}_{\mathrm{boot}}\right) \\
&=\mathbb{P}\left(z_{\alpha / 2}^{*} \leq \frac{T_{n}-\theta}{\mathrm{se}_{\mathrm{boot}}} \leq z_{1-\alpha / 2}^{*}\right) \\
&=\mathbb{P}\left(z_{\alpha / 2}^{*} \leq Z_{n} \leq z_{1-\alpha / 2}^{*}\right) \\
& \approx 1-\alpha
\end{align*}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-102}{}{\label{exr:unnamed-chunk-102} }
Note que para este caso tenemos que hacer bootstrap para cada estimador bootstrap calculado.
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\DecValTok{1000}
\NormalTok{Tboot_b <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{Tboot_bm <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{sdboot_b <-}\StringTok{ }\OtherTok{NULL}

\ControlFlowTok{for}\NormalTok{ (b }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{B) \{}
\NormalTok{    xb <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{size =}\NormalTok{ n, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    Tboot_b[b] <-}\StringTok{ }\KeywordTok{var}\NormalTok{(xb)}
    \ControlFlowTok{for}\NormalTok{ (m }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{B) \{}
\NormalTok{        xbm <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(xb, }\DataTypeTok{size =}\NormalTok{ n, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{        Tboot_bm[b] <-}\StringTok{ }\KeywordTok{var}\NormalTok{(xbm)}
\NormalTok{    \}}
\NormalTok{    sdboot_b <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(Tboot_bm)}
\NormalTok{\}}

\NormalTok{z_star <-}\StringTok{ }\NormalTok{(Tboot_b }\OperatorTok{-}\StringTok{ }\NormalTok{Tn)}\OperatorTok{/}\NormalTok{sdboot_b}

\KeywordTok{hist}\NormalTok{(z_star)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-103-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(z_star, }\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot, Tn }\OperatorTok{-}\StringTok{ }
\StringTok{    }\KeywordTok{quantile}\NormalTok{(z_star, }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    97.5%     2.5% 
## 309.8684 521.6372
\end{verbatim}

\texttt{}

\hypertarget{resumiendo}{%
\subsection{Resumiendo}\label{resumiendo}}

Resumiendo todos lo métodos de cálculo de intervalos obtenemos

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Metodo =} \KeywordTok{c}\NormalTok{(}\StringTok{"Jacknife"}\NormalTok{, }\StringTok{"Bootstrap Normal"}\NormalTok{, }
    \StringTok{"Bootstrap Pivotal"}\NormalTok{, }\StringTok{"Bootstrap Pivotal Estudentizado"}\NormalTok{), }
    \DataTypeTok{Inferior =} \KeywordTok{c}\NormalTok{(Tjack }\OperatorTok{-}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdjack}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n), Tn }\OperatorTok{-}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }
\StringTok{        }\NormalTok{sdboot, }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(Tboot_b, }\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{), }
\NormalTok{        Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(z_star, }\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot), }
    \DataTypeTok{Superior =} \KeywordTok{c}\NormalTok{(Tjack }\OperatorTok{+}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{sdjack}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n), Tn }\OperatorTok{+}\StringTok{ }\NormalTok{z }\OperatorTok{*}\StringTok{ }
\StringTok{        }\NormalTok{sdboot, }\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(Tboot_b, }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{), }
\NormalTok{        Tn }\OperatorTok{-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(z_star, }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{sdboot)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
Metodo & Inferior & Superior\\
\hline
Jacknife & 285.1679 & 573.3289\\
\hline
Bootstrap Normal & 283.8315 & 574.6653\\
\hline
Bootstrap Pivotal & 271.2827 & 551.4989\\
\hline
Bootstrap Pivotal Estudentizado & 309.8684 & 521.6372\\
\hline
\end{tabular}

\hypertarget{ejercicios}{%
\section{Ejercicios}\label{ejercicios}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Repita los ejercicios anteriores para calcular intervalos de confianza para la distancia promedio y la varianza del desplazamiento de las personas. Use los métodos de Jacknife y Bootstrap (con todos sus intervalos de confianza).
  Dada que la distancia es una medida que puede ser influenciada por distancias muy cortas o muy largas, se puede calcular el logaritmo de esta variable para eliminar la escala de la distancias.
\item
  Verifique que esta última variable se podría estimar paramétricamente con una distribución normal.
  Repita los cálculos anteriores tomando como cuantiles los de una normal con media 0 y varianza 1.
\item
  Compare los intervalos calculados y comente los resultados.
\end{enumerate}

\hypertarget{estimaciuxf3n-de-densidades-con-bayes}{%
\chapter{Estimación de densidades con Bayes}\label{estimaciuxf3n-de-densidades-con-bayes}}

\hypertarget{introducciuxf3n-a-la-estimaciuxf3n-bayesiana}{%
\section{Introducción a la estimación Bayesiana}\label{introducciuxf3n-a-la-estimaciuxf3n-bayesiana}}

\hypertarget{preliminares}{%
\subsection{Preliminares}\label{preliminares}}

Recordemos que tenemos \(f(\theta)\) la previa, \(L(\theta)\) la
verosimilitud de los datos y \(f(\theta|\text { data })\) la posterior
ajustada a los datos.

\begin{equation*}
    f(\theta | \text { data }) \propto f(\theta) L(\theta)
\end{equation*}

Además para el caso de la binomial tenemos que

\begin{equation*}
    f(y | \theta)=\theta^{\gamma}(1-\theta)^{(1-\gamma)}
\end{equation*}

y la distribución beta se escribe de la forma

\begin{align*}
    f(\theta | a, b) & =\operatorname{beta}(\theta | a, b)         \\
                     & =\theta^{(a-1)}(1-\theta)^{(b-1)} / B(a, b)
\end{align*}

donde

\begin{equation*}
    B(a, b)=\int_{0}^{1}  \theta^{(a-1)}(1-\theta)^{(b-1)}\mathrm{d} \theta.
\end{equation*}

Los valores de \(a\) y \(b\) controlan la forma de esta distribución

\begin{figure}
\centering
\includegraphics{manual_figures/betas.png}
\caption{Tomado de \textcite{Kruschke2014}}
\end{figure}

Una forma alternative es \(\mu=a /(a+b)\) es la media, \(\kappa=a+b\)
es la concentración y \(\omega=(a-1) /(a+b-2)\) es la moda de la
distribución Beta, entonces se cumple que

\begin{align*}
          & a=\mu \kappa \quad \text { y } \quad b=(1-\mu) \kappa                                         \\
          & a=\omega(\kappa-2)+1 \quad \text { y } \quad b=(1-\omega)(\kappa-2)+1 \text { para } \kappa>2
\end{align*}

Es decir, es posible estimar \(a\) y \(b\) de \(\kappa\), \(\mu\) y \(\omega\)

De acuerdo la combinación de estas dos distribuciones forma una familia conjugada de modo que

\begin{align*}
        f(\theta | z, N)
          & = f(z, N | \theta) f(\theta) / f(z, N)
        \quad                                              \\
          & = \theta^{z}(1-\theta)^{(N-z)} \frac{\theta^{(a-1)}(1-\theta)^{(b-1)}}{B(a, b)} / p(z, N) \\
          & = \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)} /[B(a, b) p(z, N)]        \\
          & = \theta^{((z+a)-1)}(1-\theta)^{((N-z+b)-1)} /[B(a, b) p(z, N)]                           \\
          & = \theta^{((z+a)-1)}(1-\theta)^{((N-z+b)-1)} / B(z+a, N-z+b)
\end{align*}

\hypertarget{ejemplo-sencillo}{%
\subsection{Ejemplo sencillo}\label{ejemplo-sencillo}}

Suponga que se hace una encuesta a 27 estudiantes y se encuentra que
11 dicen que duermen más de 8 horas diarias y el resto no. Nuestro
objetivo es encontrar inferencias sobre la proporción \(p\) de
estudiantes que duermen al menos 8 horas diarias. El modelo más
adecuado es

\[
        f(x \vert p) \propto p^s (1-p)^f
\]

donde \(s\) es la cantidad de estudiantes que duermen más de 8 horas y
\(f\) los que duermen menos de 8 horas.

Una primera aproximación para la previa es usar una distribución
discreta. En este caso, el investigador asigna una probabilidad a
cierta cantidad de horas de sueño, según su experiencia. Así, por
ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(prior <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{5.2}\NormalTok{, }\DecValTok{8}\NormalTok{, }\FloatTok{7.2}\NormalTok{, }\FloatTok{4.6}\NormalTok{, }\FloatTok{2.1}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{, }
    \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1.0 5.2 8.0 7.2 4.6 2.1 0.7 0.1 0.0 0.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(prior <-}\StringTok{ }\NormalTok{prior}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(prior))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.034602076 0.179930796 0.276816609 0.249134948 0.159169550 0.072664360
##  [7] 0.024221453 0.003460208 0.000000000 0.000000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(p, prior, }\DataTypeTok{type =} \StringTok{"h"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Probabilidad Previa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-107-1.pdf}

El paquete \texttt{LearnBayes} tiene la función \texttt{pdisc} que estima la
distribución posterior para una previa discreta binomial. Recuerde que
el valor 11 representa la cantidad de estudiantes con más de 8 horas
de sueño y 16 lo que no duermen esa cantidad.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(LearnBayes)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{post <-}\StringTok{ }\KeywordTok{pdisc}\NormalTok{(p, prior, data)}
\KeywordTok{round}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(p, prior, post), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          p prior post
##  [1,] 0.05  0.03 0.00
##  [2,] 0.15  0.18 0.00
##  [3,] 0.25  0.28 0.13
##  [4,] 0.35  0.25 0.48
##  [5,] 0.45  0.16 0.33
##  [6,] 0.55  0.07 0.06
##  [7,] 0.65  0.02 0.00
##  [8,] 0.75  0.00 0.00
##  [9,] 0.85  0.00 0.00
## [10,] 0.95  0.00 0.00
\end{verbatim}

Y podemos ver la diferencia entre la previa (negro) y la posterior
(roja),

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(p, post, }\DataTypeTok{type =} \StringTok{"h"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(p }\OperatorTok{+}\StringTok{ }\FloatTok{0.01}\NormalTok{, prior, }\DataTypeTok{type =} \StringTok{"h"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-109-1.pdf}

¿Qué se puede deducir de estos resultados?

\hypertarget{datos-reales}{%
\subsection{Datos reales}\label{datos-reales}}

Continuemos el ejercicio pero esta vez usando datos reales.

Carguemos los datos \texttt{studdendata} del paquete \texttt{LearnBayes}. Esta base
son preguntas que se le hicieron a un grupo de estudiantes de Bowling
Green State University. Para mayor información use \texttt{?studentdata}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"studentdata"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Como solo se tiene la hora de dormir y la hora de despertarse, se debe tomar la diferencia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horas_sueno <-}\StringTok{ }\NormalTok{studentdata}\OperatorTok{$}\NormalTok{WakeUp }\OperatorTok{-}\StringTok{ }\NormalTok{studentdata}\OperatorTok{$}\NormalTok{ToSleep}
\NormalTok{horas_sueno <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(horas_sueno)}
\KeywordTok{summary}\NormalTok{(horas_sueno)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   2.500   6.500   7.500   7.385   8.500  12.500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(horas_sueno, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-111-1.pdf}

Ahora supongamos que se tiene quiere ajustar una previa continua a este modelo. Para esto usaremos una distribución Beta con parámetros \(a\) y \(b\), de la forma

\[
    f(p\vert \alpha, \beta) \propto p^{1-a} (1-p)^{1-b}.
\]

El ajuste de los parámetros de la Beta depende mucho de la información
previa que se tenga del modelo. Una forma fácil de estimarlo es a
través de cuantiles con los cuales se puede reescribir estos
parámetros. En particular, suponga que se cree que el \(50\%\) de las
observaciones la proporción será menor que 0.3 y el \(90\%\) será menor
que 0.5.

Para esto ajustaremos los siguientes parámetros

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quantile2 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{x =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{quantile1 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{p =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{x =} \FloatTok{0.3}\NormalTok{)}
\NormalTok{ab <-}\StringTok{ }\KeywordTok{beta.select}\NormalTok{(quantile1, quantile2)}

\NormalTok{a <-}\StringTok{ }\NormalTok{ab[}\DecValTok{1}\NormalTok{]}
\NormalTok{b <-}\StringTok{ }\NormalTok{ab[}\DecValTok{2}\NormalTok{]}
\NormalTok{s <-}\StringTok{ }\DecValTok{11}
\NormalTok{f <-}\StringTok{ }\DecValTok{16}
\end{Highlighting}
\end{Shaded}

En este caso se obtendra la distribución posterior Beta con paramétros
\(\alpha + s\) y \(\beta + f\),

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dbeta}\NormalTok{(x, a }\OperatorTok{+}\StringTok{ }\NormalTok{s, b }\OperatorTok{+}\StringTok{ }\NormalTok{f), }\DataTypeTok{from =} \DecValTok{0}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"p"}\NormalTok{, }
    \DataTypeTok{ylab =} \StringTok{"Densidad"}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{4}\NormalTok{)}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dbeta}\NormalTok{(x, s }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, f }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }
    \DataTypeTok{lwd =} \DecValTok{4}\NormalTok{)}
\KeywordTok{curve}\NormalTok{(}\KeywordTok{dbeta}\NormalTok{(x, a, b), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{3}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{4}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Previa"}\NormalTok{, }\StringTok{"Verosimilitud"}\NormalTok{, }\StringTok{"Posterior"}\NormalTok{), }
    \DataTypeTok{lty =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-113-1.pdf}

En particular, si estamos interesados en \(\mathbb{P}(p>=.5 | \text {  data })\) se puede estimar con

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{pbeta}\NormalTok{(}\FloatTok{0.5}\NormalTok{, a }\OperatorTok{+}\StringTok{ }\NormalTok{s, b }\OperatorTok{+}\StringTok{ }\NormalTok{f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0690226
\end{verbatim}

y el intervalo de confianza correspondiente a esta distribución sería

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qbeta}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{), a }\OperatorTok{+}\StringTok{ }\NormalTok{s, b }\OperatorTok{+}\StringTok{ }\NormalTok{f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2555267 0.5133608
\end{verbatim}

Otra opción para estimar este intervalo es simular 1000 veces la
distribución beta y observar su comportamiento en los cuantiles

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps <-}\StringTok{ }\KeywordTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, a }\OperatorTok{+}\StringTok{ }\NormalTok{s, b }\OperatorTok{+}\StringTok{ }\NormalTok{f)}
\KeywordTok{hist}\NormalTok{(ps, }\DataTypeTok{xlab =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-116-1.pdf}

La probabilidad que este valor sea mayor que 0.5 es

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(ps }\OperatorTok{>=}\StringTok{ }\FloatTok{0.5}\NormalTok{)}\OperatorTok{/}\DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.069
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(ps, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        5%       95% 
## 0.2520157 0.5196835
\end{verbatim}

\hypertarget{previa-de-histograma}{%
\section{Previa de histograma}\label{previa-de-histograma}}

El caso anterior funciona perfecto dada la combinación Binomial-Beta.

¿Qué pasaría si nuestra previa no está basada beta, sino que
quisiéramos extraerla directamente de los datos?

El método que usaremos será el siguiente:

\begin{itemize}
\tightlist
\item
  Elija una cuadrícula de valores de \(p\) sobre un intervalo que cubra
  la densidad posterior.
\item
  Calcule el producto de la probabilidad \(L (p)\) y el \(f (p)\) sobre
  esa grilla.
\item
  Normalice dividiendo cada producto por la suma de los productos. En
  esto paso, estamos aproximando la densidad posterior por una
  probabilidad discreta Distribución en la grilla.
\item
  Usando el comando \texttt{sample} de \texttt{R}, tome una muestra aleatoria con
  reemplazo de la distribución discreta.
\end{itemize}

El resultado nos debe arrojar una muestra de la distribución posterior
sobre la grilla

Suponga nuevamente que tenemos las mismas previas dadas al inicio del
capítulo

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{midpt <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{prior <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{5.2}\NormalTok{, }\DecValTok{8}\NormalTok{, }\FloatTok{7.2}\NormalTok{, }\FloatTok{4.6}\NormalTok{, }\FloatTok{2.1}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{prior <-}\StringTok{ }\NormalTok{prior}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(prior)}
\end{Highlighting}
\end{Shaded}

Con la función \texttt{histprior} construye los valores de \(p\) sobre una
grilla.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{curve}\NormalTok{(}\KeywordTok{histprior}\NormalTok{(x, midpt, prior), }\DataTypeTok{from =} \DecValTok{0}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1}\NormalTok{, }
    \DataTypeTok{ylab =} \StringTok{"Densidad previa"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-120-1.pdf}

Luego recordando que nuestra posterior es \(beta(s+1,f+1)\) tenemos que

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{curve}\NormalTok{(}\KeywordTok{histprior}\NormalTok{(x, midpt, prior) }\OperatorTok{*}\StringTok{ }\KeywordTok{dbeta}\NormalTok{(x, s }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }
\NormalTok{    f }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{), }\DataTypeTok{from =} \DecValTok{0}\NormalTok{, }\DataTypeTok{to =} \DecValTok{1}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Densidad posterior"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-121-1.pdf}

Para conseguir la distribución posterior, solo debemos de construirla para una secuencia ordenada de valores \(p\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{1000}\NormalTok{)}
\NormalTok{post =}\StringTok{ }\KeywordTok{histprior}\NormalTok{(p, midpt, prior) }\OperatorTok{*}\StringTok{ }\KeywordTok{dbeta}\NormalTok{(p, s }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }
\NormalTok{    f }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{post =}\StringTok{ }\NormalTok{post}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(post)}
\end{Highlighting}
\end{Shaded}

Finalmente basta con tomar el muestreo de la posterior

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(p, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob =}\NormalTok{ post)}
\KeywordTok{hist}\NormalTok{(ps, }\DataTypeTok{xlab =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-123-1.pdf}

\hypertarget{muxe9todos-monte-carlo}{%
\section{Métodos Monte Carlo}\label{muxe9todos-monte-carlo}}

\hypertarget{una-moneda}{%
\section{Una moneda}\label{una-moneda}}

El tratamiento clásico de la estimación de parámetros bayesiana nos dice que si tenemos una densidad previa y la ``combinamos'' con la verosimilitud de los datos, estos nos dará una densidad con más información. Se podría repetir el proceso varias veces para tratar de ajustar mejor la densidad posterior.

Sin embargo, se podría usar potencia de los métodos Monte Carlo para que esta búsqueda sea muy efectiva para encontrar los parámetros adecuados.

\hypertarget{ejemplo-del-viajero}{%
\subsection{Ejemplo del viajero}\label{ejemplo-del-viajero}}

Suponga que tenemos un viajero que quiere estar en 7 lugares distintos (suponga que están en línea recta) y la probabilidad de pasar a un lugar a otro se decide tirando una moneda no sesgada (50\% a la derecha y 50\% a la izquierda).

Este caso sería una simple caminata aleatoria sin ningún interés en particular.

Suponga además, que el viajero quiere estar más tiempo donde haya una mayor cantidad de personas \(P\) pero siguiendo ese patrón aleatorio. Entonces la forma de describir su decisión de moverse sería:

\begin{itemize}
\tightlist
\item
  Tira la moneda y decide si va a la izquierda o la derecha.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Si el lugar nuevo tiene \textbf{MÁS} personas que el actual salta a ese lugar.
  \item
    Si el lugar nuevo tiene \textbf{MENOS} personas entonces el viajero tiene que decidir si se queda o se mueve. \textbar{} calcula la probabilidad de moverse como \(p_{moverse} = P_{nuevo}/P_{actual}\).

    \textbf{Tira un número aleatorio entre 0 y 1 \(r\)}

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \tightlist
    \item
      Si \(p_{moverse}>r\) entonces se mueve.
    \item
      Sino, se queda donde está.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{P <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{7}

\NormalTok{pos_actual <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(P, }\DecValTok{1}\NormalTok{)}
\NormalTok{pos_nueva <-}\StringTok{ }\NormalTok{pos_actual}

\NormalTok{n_pasos <-}\StringTok{ }\DecValTok{50000}
\NormalTok{trayectoria <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_pasos)}

\NormalTok{trayectoria[}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{pos_actual}

\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{n_pasos) \{}
    \CommentTok{# Tira la moneda para decidir}
    
\NormalTok{    moneda <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
    \CommentTok{# moneda es 0 o 1}
\NormalTok{    pos_nueva <-}\StringTok{ }\NormalTok{pos_actual}
    \ControlFlowTok{if}\NormalTok{ (moneda }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{(pos_actual }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{<=}\StringTok{ }\DecValTok{7}\NormalTok{) \{}
\NormalTok{        pos_nueva =}\StringTok{ }\NormalTok{pos_actual }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (moneda }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{(pos_actual }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{>=}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
\NormalTok{        pos_nueva <-}\StringTok{ }\NormalTok{pos_actual }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{    \}}
    
\NormalTok{    p_moverse <-}\StringTok{ }\KeywordTok{min}\NormalTok{(pos_nueva}\OperatorTok{/}\NormalTok{pos_actual, }\DecValTok{1}\NormalTok{)}
    
\NormalTok{    hay_movimiento <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{p_moverse }\OperatorTok{<=}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}
    
    \ControlFlowTok{if}\NormalTok{ (hay_movimiento) \{}
\NormalTok{        pos_actual <-}\StringTok{ }\NormalTok{pos_nueva}
\NormalTok{    \}}
    
\NormalTok{    trayectoria[k] <-}\StringTok{ }\NormalTok{pos_nueva}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\OperatorTok{:}\NormalTok{n_pasos, }\DataTypeTok{P =}\NormalTok{ trayectoria)}

\KeywordTok{ggplot}\NormalTok{(df[}\DecValTok{1}\OperatorTok{:}\DecValTok{200}\NormalTok{, ]) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, P)) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-125-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(P), }\DataTypeTok{stat =} \StringTok{"count"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-125-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(trayectoria)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.86008
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(trayectoria)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.798412
\end{verbatim}

\hypertarget{cadenas-de-markov}{%
\subsection{Cadenas de Markov}\label{cadenas-de-markov}}

Recuerde que estamos buscando el \enquote{camino} que el viajero tomará para pasar la mayor parte del tiempo de en los lugares más poblados (con mayor \(\theta\)).

\begin{equation*}
\theta_{1} \curvearrowright \theta_{2} \curvearrowright \ldots \curvearrowright \theta_{50000}.
\end{equation*}

Denotamos \(\theta_{1}\to\theta_{2}\) si el viajero pasó de \(\theta_{1}\) hacia \(\theta_{2}\).

Entonces

\begin{itemize}
\tightlist
\item
  \(\mathbb{P}\left(\theta \rightarrow \theta+1\right)=0.5 \min \left(\frac{P(\theta+1)}{P(\theta)} / , 1\right)\)
\item
  \(\mathbb{P}\left(\theta + 1 \rightarrow \theta\right)=0.5 \min \left(\frac{P(\theta)}{P(\theta+1)} / , 1\right)\)
\end{itemize}

Entonces la razón entre estas dos probabilidades es

\begin{align*}
    \frac{\mathbb{P}\left(\theta \rightarrow \theta+1\right)}{\mathbb{P}\left(\theta +1 \rightarrow \theta\right)} &=\frac{0.5 \min (P(\theta+1) / P(\theta), 1)}{0.5 \min (P(\theta) / P(\theta+1), 1)} \\
    &=\left\{\begin{array}{ll}
        \frac{P(\theta+1)}{P(\theta) } & \text { si } P(\theta+1)>P(\theta) \\
        \frac{P(\theta+1) }{P(\theta)} & \text { si } P(\theta+1)<P(\theta)
    \end{array}\right.\\
    &=\frac{P(\theta+1)}{P(\theta)}.
\end{align*}

Es decir que la razón de las probabilidades es equivalente a la razón entre las proporción de las poblaciones. Por lo tanto la mayoría de las veces se estará en los lugares con mayor población.

Esta cadena se puede escribir usando una matriz de transición de la forma

\begin{equation*}
T= \left(\begin{array}{ccccc}
\ddots & \mathbb{P}(\theta-2 \rightarrow \theta-1) & 0 & 0 & 0 \\
\ddots & \mathbb{P}(\theta-1 \rightarrow \theta-1) & \mathbb{P}(\theta-1 \rightarrow \theta) & 0 & 0 \\
0 & \mathbb{P}(\theta \rightarrow \theta-1) & \mathbb{P}(\theta \rightarrow \theta) & \mathbb{P}(\theta \rightarrow \theta+1) & 0 \\
0 & 0 & \mathbb{P}(\theta+1 \rightarrow \theta) & \mathbb{P}(\theta+1 \rightarrow \theta+1) & \ddots \\
0 & 0 & 0 & \mathbb{P}(\theta+2 \rightarrow \theta+1) & \ddots
\end{array}\right)
\end{equation*}

La matriz \(T\) tiene las propiedades

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Existencia de una única distribución estacionaria (llamada \(f\) más adelante).
\item
  Es ergódica, i.e., es aperíodica y positiva recurrente. Recuerde que una cadena de markov es érgodica si siempre se puede pasar de un estado a otro (no necesariamente en 1 paso). Otra forma de verlo es que la para alguna potencia de \(T\) todos los sus elementos serán positivos estrictos.
\end{enumerate}

\hypertarget{el-algoritmo-de-metropolis-hasting}{%
\subsection{El algoritmo de Metropolis-Hasting}\label{el-algoritmo-de-metropolis-hasting}}

El ejemplo anterior era bastante sencillo pero demuestra que se puede
encontrar el mejor estimador posible simplemente ejecutando una y otra
vez maximizando la estadía en los lugares más poblados.

En este ejemplo la función a maximizar es la cantidad de personas
\(P(\theta)=\theta\), pero en general nuestro objetivo será maximizar
la distribución posterior \(f(\theta| \text{ datos })\).

En palabras simples el algoritmo de Metropoli Hasting es

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simule un valor \(\theta^{*}\) de una densidad de propuesta
  \(p\left(\theta^{*} | \theta^{t-1}\right)\)
\item
  Estime la razón
  \[
   R=\frac{f\left(\theta^{*}\right) L\left(\theta^{t-1} |
       \theta^{*}\right)}{f\left(\theta^{t-1}\right) L\left(\theta^{*} |
       \theta^{t-1}\right)}
  \]
\item
  Estima la probabilidad de aceptación \(p_{\text {moverse }}=\min \{R, 1\}\).
\item
  Tome \(\theta^{t}\) tal que \(\theta^{t}=\theta^{*}\)
  con probabilidad \(p_{\text {moverse }}\); en otro caso \(\theta^{t}=\) \(\theta^{t-1}\)
\end{enumerate}

El algoritmo de Metropolis-Hastings se puede construir de muchas
formas, dependiendo de la densidad de proposición

Si esta es independiente de las elecciones anteriores entonces,
\[
    L\left(\theta^{*} | \theta^{t-1}\right)=L\left(\theta^{*}\right)
\]

Otras formas es escoger
\[
    L\left(\theta^{*} |
    \theta^{t-1}\right)=h\left(\theta^{*}-\theta^{t-1}\right)
\]
donde \(h\) es simétrica alrededor del origen. En este tipo de
cadenas, la razón \(R\) tiene la forma
\[
    R=\frac{f\left(\theta^{*}\right)}{f\left(\theta^{t-1}\right)}
\]

Una última opción es tomar
\[
    \theta^{*}=\theta^{t-1}+ Z
\]

donde \(Z\) es una normal centrada con cierta estructura de varianza.

\hypertarget{por-quuxe9-el-algoritmo-de-metropolis-hasting-funciona}{%
\subsection{¿Por qué el algoritmo de Metropolis Hasting funciona?}\label{por-quuxe9-el-algoritmo-de-metropolis-hasting-funciona}}

\begin{equation}
\mathbb{P}\left(\theta_{\star} | \theta^{(t)}\right)=L\left(\theta_{\star} | \theta^{(t)}\right) \cdot \min \left\{1, \frac{f\left(\theta_{\star}\right) L\left(\theta^{(t)} | \theta_{\star}\right)}{f\left(\theta^{(t)}\right) L\left(\theta_{\star} | \theta^{(t)}\right)}\right\}
\end{equation}

Si se comienza en \(f\left(\theta^{(t)}\right)\) entonces

\begin{align}
\begin{array}{l}
f\left(\theta^{(t)}\right) \mathbb{P}\left(\theta_{\star} | \theta^{(t)}\right) \\
\quad=\quad f\left(\theta^{(t)}\right) L\left(\theta_{\star} | \theta^{(t)}\right) \min \left\{1, \frac{f\left(\theta_{\star}\right) L\left(\theta^{(t)} | \theta_{\star}\right)}{f\left(\theta^{(t)}\right) L\left(\theta_{\star} | \theta^{(t)}\right)}\right\} \\
\quad=\min \left\{f\left(\theta^{(t)}\right) L\left(\theta_{\star} | \theta^{(t)}\right), f\left(\theta_{\star}\right) L\left(\theta^{(t)} | \theta_{\star}\right)\right\} \\
\quad=\quad f\left(\theta_{\star}\right) L\left(\theta^{(t)} | \theta_{\star}\right) \min \left\{\frac{f\left(\theta^{(t)}\right) L\left(\theta_{\star} | \theta^{(t)}\right)}{f\left(\theta_{\star}\right) L\left(\theta^{\left(t | \theta_{\star}\right)}\right.}, 1\right\} \\
\quad=f\left(\theta_{\star}\right) \mathbb{P}\left(\theta^{(t)} | \theta_{\star}\right)
\end{array}
\end{align}

Asumiendo que existe una cantidad finita de estados \(\theta_{1}, \ldots, \theta_{M}\), entonces.

\begin{equation*}
f\left(\theta_{j}\right) = \underbrace{\sum_{i=1}^{M} f\left(\theta_{i}\right) \mathbb{P} \left(\theta_{j} | \theta_{i}\right)}_{\text {Probabilidad total  }}=\sum_{i=1}^{M} f\left(\theta_{j}\right) \mathbb{P} \left(\theta_{i} | \theta_{j}\right)
\end{equation*}

Cual indica que no importa donde empecemos siempre llegaremos a la densidad estacionaria \(f\).

\href{https://www.ece.iastate.edu/~namrata/EE527_Spring08/l4c.pdf\#page=32}{Para más detalles}

\hypertarget{extensiuxf3n-al-caso-del-viajero}{%
\subsection{Extensión al caso del viajero}\label{extensiuxf3n-al-caso-del-viajero}}

Retomemos el ejemplo del viajero. Supongamos que ahora existen una
cantidad infinita de lugares a los que puede ir y que la población de
cada isla es proporcional a la densidad posterior. Además, el viajero
podría saltar a cualquier isla que quisiera y su probabilidad de
salto cae de forma continua en el intervalo \([0,1]\).

Para hacer este ejemplo concreto, el viajero no conoce cuál es su
probabilidad de salto \(\theta\) pero sabe que ha tirado la
moneda \(N\) veces y observado \(z\) exitos. Por lo tanto tendremos
una verosimilitud de \(L(z, N | \theta)=\theta^{z}(1-\theta)^{(N-z)}\).

La previa será dada por \(f(\theta)=\operatorname{beta}(\theta | a, b)\).

Los saltos serán gobernados por una normal centrada con media
\(\sigma\) de modo que \(\Delta \theta \sim \mathcal{N}\left(0,\sigma^{2}\right)\).

Entonces el algoritmo de Metropolis Hasting se puede reformular como

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simule un valor de salto\(\Delta \theta \sim \mathcal{N}\left(0,\sigma^{2}\right)\) y denote \(\theta^{t} = \theta^{t} + \Delta\theta\).
\item
  Probabilidad de aceptación \(p_{\text {moverse }}\)
  \begin{align*}
   p_{\text {moverse }}
     & =\min \left(1, \frac{P\left(\theta_{\ast}\right)}{P\left(\theta_{t-1}\right)}\right) \\
                 & =\min \left(1, \frac{p\left(D |
       \theta_{\ast}\right) p\left(\theta_{\ast}\right)}{p\left(D | \theta_{t-1}\right)
       p\left(\theta_{t-1}\right)}\right) \\
     & =\min \left(1, \frac{\operatorname{Bernoulli}\left(z, N |
       \theta_{\ast}\right)
       \operatorname{beta}\left(\theta_{\ast} | a,
       b\right)}{\operatorname{Bernoulli}\left(z, N |
       \theta_{t-1}\right)
       \operatorname{beta}\left(\theta_{t-1} | a, b\right)}\right) \\
     & =\min \left(1,
   \frac{\theta_{\ast}^{z}\left(1-\theta_{\ast}\right)^{(N-z)}
           \theta_{\ast} \left(1-\theta_{\ast}\right)^{(b-1)}
           / B(a,b)}{\theta_{t-1}^{z}\left(1-\theta_{t-1}\right)^{(N-z)}
           \theta_{t-1}^{(a-1)}\left(1-\theta_{t-1}\right)^{(b-1)}
           / B(a, b)}\right)
  \end{align*}
\item
  Tome \(\theta_{t}\) tal que \(\theta_{t}=\theta_{*}\)
  con probabilidad \(p_{\text {moverse }} ;\) en otro caso \(\theta_{t}=\) \(\theta_{t-1}\)
\end{enumerate}

En el ejemplo del viajero queremos ver la probabilidad \(\theta\) de
que salte al siguiente destino. Tomemos \(\sigma=0.2\) y supongamos
que se ha visto que el viajero de \(N=20\) y \(z=14\) éxitos. Por
cuestiones de practicidad se tomará \(\theta_0 = 0.1\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Carga de datos observados}
\NormalTok{datos_observados <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{14}\NormalTok{))}

\CommentTok{# Función de verosimilitud Binomial}
\NormalTok{verosimilitud <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(theta, data) \{}
\NormalTok{    z <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(data)}
\NormalTok{    N <-}\StringTok{ }\KeywordTok{length}\NormalTok{(data)}
\NormalTok{    pDatosDadoTheta <-}\StringTok{ }\NormalTok{theta}\OperatorTok{^}\NormalTok{z }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{theta)}\OperatorTok{^}\NormalTok{(N }\OperatorTok{-}\StringTok{ }\NormalTok{z)}
    \CommentTok{# Es para asegurarse que los datos caigan en [0,1].}
\NormalTok{    pDatosDadoTheta[theta }\OperatorTok{>}\StringTok{ }\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{theta }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
    \KeywordTok{return}\NormalTok{(pDatosDadoTheta)}
\NormalTok{\}}

\CommentTok{# densidad previa}
\NormalTok{previa <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(theta) \{}
\NormalTok{    pTheta <-}\StringTok{ }\KeywordTok{dbeta}\NormalTok{(theta, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
    \CommentTok{# Es para asegurarse que los datos caigan en [0,1].}
\NormalTok{    pTheta[theta }\OperatorTok{>}\StringTok{ }\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{theta }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
    \KeywordTok{return}\NormalTok{(pTheta)}
\NormalTok{\}}

\CommentTok{# densidad posterior}
\NormalTok{posterior <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(theta, data) \{}
\NormalTok{    posterior <-}\StringTok{ }\KeywordTok{verosimilitud}\NormalTok{(theta, data) }\OperatorTok{*}\StringTok{ }\KeywordTok{previa}\NormalTok{(theta)}
    \KeywordTok{return}\NormalTok{(posterior)}
\NormalTok{\}}

\NormalTok{n_pasos <-}\StringTok{ }\DecValTok{50000}

\NormalTok{trayectoria <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_pasos)}

\CommentTok{# Valor inicial}
\NormalTok{trayectoria[}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\FloatTok{0.01}

\NormalTok{n_aceptados <-}\StringTok{ }\DecValTok{0}
\NormalTok{n_rechazados <-}\StringTok{ }\DecValTok{0}


\NormalTok{sigma <-}\StringTok{ }\FloatTok{0.2}

\ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{(n_pasos }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{)) \{}
\NormalTok{    pos_actual <-}\StringTok{ }\NormalTok{trayectoria[t]}
    
\NormalTok{    salto_propuesto <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =}\NormalTok{ sigma)}
    
\NormalTok{    proba_aceptacion <-}\StringTok{ }\KeywordTok{min}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{posterior}\NormalTok{(pos_actual }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{salto_propuesto, datos_observados)}\OperatorTok{/}\KeywordTok{posterior}\NormalTok{(pos_actual, }
\NormalTok{        datos_observados))}
    
    \CommentTok{# Aceptamos el salto?}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{<}\StringTok{ }\NormalTok{proba_aceptacion) \{}
        \CommentTok{# Aceptados}
\NormalTok{        trayectoria[t }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{pos_actual }\OperatorTok{+}\StringTok{ }\NormalTok{salto_propuesto}
\NormalTok{        n_aceptados <-}\StringTok{ }\NormalTok{n_aceptados }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \CommentTok{# Rechazos}
\NormalTok{        trayectoria[t }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{pos_actual}
\NormalTok{        n_rechazados <-}\StringTok{ }\NormalTok{n_rechazados }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Obtenemos una tasa de aceptación del 49.4 y tasa de rechazo del 50.59

Podemos desechar los primeros 500 pasos (por ejemplo) del proceso ya que estos son de \enquote{calentamiento}. De esta forma podremos estimar la media y la varianza de las trayectoria.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(trayectoria[}\DecValTok{500}\OperatorTok{:}\NormalTok{n_pasos])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6808914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(trayectoria[}\DecValTok{500}\OperatorTok{:}\NormalTok{n_pasos])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.09721105
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \DecValTok{1}\OperatorTok{:}\NormalTok{n_pasos, }\DataTypeTok{P =}\NormalTok{ trayectoria)}

\KeywordTok{ggplot}\NormalTok{(df[}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, ]) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, P), }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-129-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df[}\DecValTok{500}\OperatorTok{:}\NormalTok{n_pasos, ]) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(P, }\DataTypeTok{y =}\NormalTok{ ..density..), }
    \DataTypeTok{color =} \StringTok{"white"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-129-2.pdf}

\hypertarget{dos-monedas}{%
\section{Dos monedas}\label{dos-monedas}}

Un problema con el algoritmo de Metropolis-Hastings (M-H) es que solo funciona para la estimación de un solo parámetro.

El muestreo de Gibbs está pensado en el caso de la estimación de muchos parámetros de forma bastante ordenada.

Supongamos que tenemos dos monedas y queremos ver la proporción de escudos generados entre las dos monedas:

Tenemos:

\begin{itemize}
\tightlist
\item
  Parámetros: \(\theta_{1}\) y \(\theta_{2}\).
\item
  Datos: \(N_{1}\) tiradas de la moneda 1 y \(N_{2}\) tiradas de la moneda 2. (cada una tuvo \(z_{1}\) y \(z_{2}\) éxitos).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Verosimilitud: Bernoulli.
  \begin{equation*}
  y_{1}^{i}\sim \mathrm{Bernoulli}(\theta_{1}) 
  \quad 
  y_{2}^{i}\sim \mathrm{Bernoulli}(\theta_{2}) 
  \end{equation*}
\item
  Previa: Beta independiente para cada \(\theta\).
  \begin{equation*}
  \theta_{1}\sim \mathrm{Beta}(a_{1},b_{1}) 
  \quad 
  \theta_{2}\sim \mathrm{Beta}(a_{2},b_{2}) 
  \end{equation*}
\end{enumerate}

La distribución posterior se puede escribir como

\begin{align*}
f\left(\theta_{1}, \theta_{2} | D\right) &=f\left(D | \theta_{1}, \theta_{2}\right) f\left(\theta_{1}, \theta_{2}\right) / f(D) \\
&=\theta_{1}^{z_{1}}\left(1-\theta_{1}\right)^{N_{1}-z_{1}} \theta_{1}^{z_{2}}\left(1-\theta_{2}\right)^{N_{2}-z_{2}} f\left(\theta_{1}, \theta_{2}\right) / f(D) \\
&=\frac{\theta_{1}^{z_{1}}\left(1-\theta_{1}\right)^{N_{1}-z_{1}} \theta_{1}^{z_{2}}\left(1-\theta_{2}\right)^{N_{2}-z_{2}} \theta_{1}^{a_{1}-1}\left(1-\theta_{1}\right)^{b_{1}-1} \theta_{2}^{a_{2}-1}\left(1-\theta_{2}\right)^{b_{2}-1}}{f(D) B\left(a_{1}, b_{1}\right) B\left(a_{2}, b_{2}\right)} \\
&=\frac{\theta_{1}^{z_{1}+a_{1}-1}\left(1-\theta_{1}\right)^{N_{1}-z_{1}+b_{1}-1} \theta_{2}^{z_{2}+a_{2}-1}\left(1-\theta_{2}\right)^{N_{2}-z_{2}+b_{2}-1}}{f(D) B\left(a_{1}, b_{1}\right) B\left(a_{2}, b_{2}\right)}
\end{align*}

Entonces la distribución posterior de \(\left(\theta_{1}, \theta_{2}\right)\) son dos distribuciones independientes Betas:
\(\operatorname{Beta}\left(z_{1}+a, N_{1}-z_{1}+b_{1}\right)\) y \(\operatorname{Beta}\left(z_{2}+a, N_{2}-z_{2}+b_{2}\right)\)

Tratemos de encontrar los parámetros para la distribución posterior usando un algoritmo de Metropolis-Hasting. \href{https://rpruim.github.io/Kruschke-Notes/markov-chain-monte-carlo-mcmc.html\#metropolis}{Función tomada de Kruschke-Notes}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{metro_2coins <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}
\NormalTok{  z1, n1,              }\CommentTok{# z = successes, n = trials}
\NormalTok{  z2, n2,              }\CommentTok{# z = successes, n = trials}
  \DataTypeTok{size  =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\CommentTok{# sds of jump distribution}
  \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\CommentTok{# value of thetas to start at}
  \DataTypeTok{num_steps =} \FloatTok{5e4}\NormalTok{,     }\CommentTok{# number of steps to run the algorithm}
  \DataTypeTok{prior1 =}\NormalTok{ dbeta,      }\CommentTok{# function describing prior}
  \DataTypeTok{prior2 =}\NormalTok{ dbeta,      }\CommentTok{# function describing prior}
  \DataTypeTok{args1 =} \KeywordTok{list}\NormalTok{(),      }\CommentTok{# additional args for prior1}
  \DataTypeTok{args2 =} \KeywordTok{list}\NormalTok{()      }\CommentTok{# additional args for prior2}
\NormalTok{  ) \{}
  
\NormalTok{  theta1            <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  theta2            <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  proposed_theta1   <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  proposed_theta2   <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  move              <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  theta1[}\DecValTok{1}\NormalTok{]         <-}\StringTok{ }\NormalTok{start[}\DecValTok{1}\NormalTok{]}
\NormalTok{  theta2[}\DecValTok{1}\NormalTok{]         <-}\StringTok{ }\NormalTok{start[}\DecValTok{2}\NormalTok{]}

\NormalTok{  size1 <-}\StringTok{ }\NormalTok{size[}\DecValTok{1}\NormalTok{] }
\NormalTok{  size2 <-}\StringTok{ }\NormalTok{size[}\DecValTok{2}\NormalTok{] }
  
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{(num_steps}\DecValTok{-1}\NormalTok{)) \{}
    \CommentTok{# head to new "island"}
\NormalTok{    proposed_theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, theta1[i], size1)}
\NormalTok{    proposed_theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, theta2[i], size2)}
    
    \ControlFlowTok{if}\NormalTok{ (proposed_theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{0} \OperatorTok{||}
\StringTok{        }\NormalTok{proposed_theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] }\OperatorTok{>=}\StringTok{ }\DecValTok{1} \OperatorTok{||}
\StringTok{        }\NormalTok{proposed_theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] }\OperatorTok{<=}\StringTok{ }\DecValTok{0} \OperatorTok{||}
\StringTok{        }\NormalTok{proposed_theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] }\OperatorTok{>=}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
\NormalTok{      proposed_posterior <-}\StringTok{ }\DecValTok{0}  \CommentTok{# because prior is 0}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      current_prior <-}\StringTok{ }
\StringTok{        }\KeywordTok{do.call}\NormalTok{(prior1, }\KeywordTok{c}\NormalTok{(}\KeywordTok{list}\NormalTok{(theta1[i]), args1)) }\OperatorTok{*}
\StringTok{        }\KeywordTok{do.call}\NormalTok{(prior2, }\KeywordTok{c}\NormalTok{(}\KeywordTok{list}\NormalTok{(theta2[i]), args2))}
\NormalTok{      current_likelihood  <-}\StringTok{ }
\StringTok{        }\KeywordTok{dbinom}\NormalTok{(z1, n1, theta1[i]) }\OperatorTok{*}
\StringTok{        }\KeywordTok{dbinom}\NormalTok{(z2, n2, theta2[i])}
\NormalTok{      current_posterior   <-}\StringTok{ }\NormalTok{current_prior }\OperatorTok{*}\StringTok{ }\NormalTok{current_likelihood}
      
\NormalTok{      proposed_prior <-}\StringTok{ }
\StringTok{        }\KeywordTok{do.call}\NormalTok{(prior1, }\KeywordTok{c}\NormalTok{(}\KeywordTok{list}\NormalTok{(proposed_theta1[i}\OperatorTok{+}\DecValTok{1}\NormalTok{]), args1)) }\OperatorTok{*}
\StringTok{        }\KeywordTok{do.call}\NormalTok{(prior2, }\KeywordTok{c}\NormalTok{(}\KeywordTok{list}\NormalTok{(proposed_theta2[i}\OperatorTok{+}\DecValTok{1}\NormalTok{]), args2))}
\NormalTok{      proposed_likelihood  <-}\StringTok{ }
\StringTok{        }\KeywordTok{dbinom}\NormalTok{(z1, n1, proposed_theta1[i}\OperatorTok{+}\DecValTok{1}\NormalTok{]) }\OperatorTok{*}
\StringTok{        }\KeywordTok{dbinom}\NormalTok{(z2, n2, proposed_theta2[i}\OperatorTok{+}\DecValTok{1}\NormalTok{])}
\NormalTok{      proposed_posterior   <-}\StringTok{ }\NormalTok{proposed_prior }\OperatorTok{*}\StringTok{ }\NormalTok{proposed_likelihood}
\NormalTok{    \}}
\NormalTok{    prob_move           <-}\StringTok{ }\NormalTok{proposed_posterior }\OperatorTok{/}\StringTok{ }\NormalTok{current_posterior}
    
    \CommentTok{# sometimes we "sail back"}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{>}\StringTok{ }\NormalTok{prob_move) \{ }\CommentTok{# sail back}
\NormalTok{       move[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\OtherTok{FALSE}
\NormalTok{      theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{theta1[i]}
\NormalTok{      theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{theta2[i]}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{                    }\CommentTok{# stay}
\NormalTok{       move[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\OtherTok{TRUE}
\NormalTok{      theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{proposed_theta1[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{]}
\NormalTok{      theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{proposed_theta2[i }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{]}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \KeywordTok{tibble}\NormalTok{(}
    \DataTypeTok{step =} \DecValTok{1}\OperatorTok{:}\NormalTok{num_steps, }
    \DataTypeTok{theta1 =}\NormalTok{ theta1,}
    \DataTypeTok{theta2 =}\NormalTok{ theta2,}
    \DataTypeTok{proposed_theta1 =}\NormalTok{ proposed_theta1,}
    \DataTypeTok{proposed_theta2 =}\NormalTok{ proposed_theta2,}
    \DataTypeTok{move =}\NormalTok{ move, }
    \DataTypeTok{size1 =}\NormalTok{ size1,}
    \DataTypeTok{size2 =}\NormalTok{ size2}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsA <-}\StringTok{ }\KeywordTok{metro_2coins}\NormalTok{(}\DataTypeTok{z1 =} \DecValTok{6}\NormalTok{, }\DataTypeTok{n1 =} \DecValTok{8}\NormalTok{, }\DataTypeTok{z2 =} \DecValTok{2}\NormalTok{, }
    \DataTypeTok{n2 =} \DecValTok{7}\NormalTok{, }\DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.02}\NormalTok{, }\FloatTok{0.02}\NormalTok{), }\DataTypeTok{args1 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{shape1 =} \DecValTok{2}\NormalTok{, }
        \DataTypeTok{shape2 =} \DecValTok{2}\NormalTok{), }\DataTypeTok{args2 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{shape1 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{shape2 =} \DecValTok{2}\NormalTok{))}

\NormalTok{Metro_2coinsA }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density2d}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-131-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsA }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density}\NormalTok{(}\OperatorTok{~}\NormalTok{(theta2 }\OperatorTok{-}\StringTok{ }\NormalTok{theta1))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-131-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(Metro_2coinsA}\OperatorTok{$}\NormalTok{theta2 }\OperatorTok{-}\StringTok{ }\NormalTok{Metro_2coinsA}\OperatorTok{$}\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-131-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsA }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }
\StringTok{    }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }
    \DataTypeTok{angle =} \DecValTok{30}\NormalTok{, }\DataTypeTok{length =} \KeywordTok{unit}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\StringTok{"inches"}\NormalTok{))) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-132-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gganimate)}
\NormalTok{Metro_2coinsAplot <-}\StringTok{ }\NormalTok{Metro_2coinsA }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }
\StringTok{    }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }
    \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }\DataTypeTok{angle =} \DecValTok{30}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{transition_reveal}\NormalTok{(step)}

\KeywordTok{animate}\NormalTok{(Metro_2coinsAplot, }\DataTypeTok{fps =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsB <-}\StringTok{ }\KeywordTok{metro_2coins}\NormalTok{(}\DataTypeTok{z1 =} \DecValTok{6}\NormalTok{, }\DataTypeTok{n1 =} \DecValTok{8}\NormalTok{, }\DataTypeTok{z2 =} \DecValTok{2}\NormalTok{, }
    \DataTypeTok{n2 =} \DecValTok{7}\NormalTok{, }\DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\DataTypeTok{args1 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{shape1 =} \DecValTok{2}\NormalTok{, }
        \DataTypeTok{shape2 =} \DecValTok{2}\NormalTok{), }\DataTypeTok{args2 =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{shape1 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{shape2 =} \DecValTok{2}\NormalTok{))}

\NormalTok{Metro_2coinsB }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density2d}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-135-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(Metro_2coinsB}\OperatorTok{$}\NormalTok{theta2 }\OperatorTok{-}\StringTok{ }\NormalTok{Metro_2coinsB}\OperatorTok{$}\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-135-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsB }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }
\StringTok{    }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }
    \DataTypeTok{angle =} \DecValTok{30}\NormalTok{, }\DataTypeTok{length =} \KeywordTok{unit}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\StringTok{"inches"}\NormalTok{))) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-136-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metro_2coinsBplot <-}\StringTok{ }\NormalTok{Metro_2coinsA }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }
\StringTok{    }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }
    \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }\DataTypeTok{angle =} \DecValTok{30}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{transition_reveal}\NormalTok{(step)}

\KeywordTok{animate}\NormalTok{(Metro_2coinsBplot, }\DataTypeTok{fps =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{muestreo-de-gibbs}{%
\subsection{Muestreo de Gibbs}\label{muestreo-de-gibbs}}

Para este ejemplo \(\left( \theta_{1},\theta_{2} \right)\), entonces la forma de escoger la posteriores en cada paso sería de la forma:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tome al azar un \(\boldsymbol{\theta}^{0} = \left( \theta_{1}^{0},\theta_{2}^{0} \right)\).
\item
  Escoja \(\theta_{1}^{1}\) a partir de la distribución
  \(f\left(\theta_{1} \vert \theta_{1}=\theta_{1}^{0}, \theta_{2}=\theta_{2}^{0}, \boldsymbol{X} \right)\).
\item
  Escoja \(\theta_{2}^{1}\) a partir de la distribución
  \(f\left(\theta_{2} \vert \theta_{1}=\theta_{1}^{1}, \theta_{2}=\theta_{2}^{0}, \boldsymbol{X} \right)\).
\end{enumerate}

Esto completa un ciclo del muestreo. Cada ciclo genera nuevos \(\boldsymbol{\theta}^{i} = \left( \theta_{1}^{i},\theta_{2}^{i} \right)\) hasta que el proceso converja.

\BeginKnitrBlock{remark}
\iffalse{} {Nota: } \fi{}En realidad el muestreo de Gibbs se basa en el algoritmo de M-H, con la diferencia que la elección de los parámetros se escogen teniendo en cuanta los datos y fijando los otros parámetros. Es decir,

\begin{align*}
&{\left[\theta_{1} | \theta_{2}, \ldots, \theta_{M}, datos \right]} \\
&{\left[\theta_{2} | \theta_{1}, \theta_{3}, \ldots, \theta_{M}, datos \right]} \\
&{\left[\theta_{M} | \theta_{1}, \ldots, \theta_{M-1}, datos \right]}
\end{align*}
\EndKnitrBlock{remark}

El tratamiento teórico puede ser consultado \href{https://www.ece.iastate.edu/~namrata/EE527_Spring08/l4c.pdf\#page=16}{aquí}

\begin{align*}\begin{aligned}
f\left(\theta_{1} | \theta_{2}, D\right) &=f\left(\theta_{1}, \theta_{2} | D\right) / f\left(\theta_{2} | D\right) \\
&=f\left(\theta_{1}, \theta_{2} | D\right) \int f\left(\theta_{1}, \theta_{2} | D\right)d \theta_{1}  \\
&=\frac{\operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right) \cdot \operatorname{dbeta}\left(\theta_{2}, z_{2}+a_{2}, N_{2}-z_{2}+b_{2}\right)}{\int  \operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right) \cdot \operatorname{dbeta}\left(\theta_{2} | z_{2}+a_{2}, N_{2}-z_{2}+b_{2}\right)d \theta_{1}} \\
&=\frac{\operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right) \cdot \operatorname{dbeta}\left(\theta_{2}, z_{2}+a_{2}, N_{2}-z_{2}+b_{2}\right)}{\operatorname{dbeta}\left(\theta_{2} | z_{2}+a_{2}, N_{2}-z_{2}+b_{2}\right) \int  \operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right)d \theta_{1}} \\
&=\frac{\operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right)}{\int  \operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right)d \theta_{1}} \\
&=\operatorname{dbeta}\left(\theta_{1}, z_{1}+a_{1}, N_{1}-z_{1}+b_{1}\right)
\end{aligned}\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gibbs_2coins <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}
\NormalTok{  z1, n1,              }\CommentTok{# z = successes, n = trials}
\NormalTok{  z2, n2,              }\CommentTok{# z = successes, n = trials}
  \DataTypeTok{start =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\CommentTok{# value of thetas to start at}
  \DataTypeTok{num_steps =} \FloatTok{1e4}\NormalTok{,     }\CommentTok{# number of steps to run the algorithm}
\NormalTok{  a1, b1,              }\CommentTok{# params for prior for theta1}
\NormalTok{  a2, b2               }\CommentTok{# params for prior for theta2}
\NormalTok{  ) \{}
  
\NormalTok{  theta1            <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  theta2            <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, num_steps)  }\CommentTok{# trick to pre-alocate memory}
\NormalTok{  theta1[}\DecValTok{1}\NormalTok{]         <-}\StringTok{ }\NormalTok{start[}\DecValTok{1}\NormalTok{]}
\NormalTok{  theta2[}\DecValTok{1}\NormalTok{]         <-}\StringTok{ }\NormalTok{start[}\DecValTok{2}\NormalTok{]}
  
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{(num_steps}\DecValTok{-1}\NormalTok{)) \{}
    \ControlFlowTok{if}\NormalTok{ (i }\OperatorTok{%%}\StringTok{ }\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{ }\CommentTok{# update theta1}
\NormalTok{      theta1[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{rbeta}\NormalTok{(}\DecValTok{1}\NormalTok{, z1 }\OperatorTok{+}\StringTok{ }\NormalTok{a1, n1 }\OperatorTok{-}\StringTok{ }\NormalTok{z1 }\OperatorTok{+}\StringTok{ }\NormalTok{b1)}
\NormalTok{      theta2[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{theta2[i]}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{           }\CommentTok{# update theta2}
\NormalTok{      theta1[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{theta1[i]}
\NormalTok{      theta2[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{rbeta}\NormalTok{(}\DecValTok{1}\NormalTok{, z2 }\OperatorTok{+}\StringTok{ }\NormalTok{a2, n2 }\OperatorTok{-}\StringTok{ }\NormalTok{z2 }\OperatorTok{+}\StringTok{ }\NormalTok{b2)}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \KeywordTok{tibble}\NormalTok{(}
    \DataTypeTok{step =} \DecValTok{1}\OperatorTok{:}\NormalTok{num_steps, }
    \DataTypeTok{theta1 =}\NormalTok{ theta1,}
    \DataTypeTok{theta2 =}\NormalTok{ theta2,}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs <-}\StringTok{ }\KeywordTok{gibbs_2coins}\NormalTok{(}\DataTypeTok{z1 =} \DecValTok{6}\NormalTok{, }\DataTypeTok{n1 =} \DecValTok{8}\NormalTok{, }\DataTypeTok{z2 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{n2 =} \DecValTok{7}\NormalTok{, }
    \DataTypeTok{a1 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{b1 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{a2 =} \DecValTok{2}\NormalTok{, }\DataTypeTok{b2 =} \DecValTok{2}\NormalTok{)}

\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density2d}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-140-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_dens}\NormalTok{(}\OperatorTok{~}\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-140-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acf}\NormalTok{(Gibbs}\OperatorTok{$}\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-140-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1, }
    \DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }
        \DataTypeTok{angle =} \DecValTok{30}\NormalTok{, }\DataTypeTok{length =} \KeywordTok{unit}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\StringTok{"inches"}\NormalTok{))) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-141-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbsplot <-}\StringTok{ }\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }
\StringTok{    }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }
    \DataTypeTok{angle =} \DecValTok{30}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{transition_reveal}\NormalTok{(step)}

\KeywordTok{animate}\NormalTok{(Gibbsplot, }\DataTypeTok{fps =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ciclos completos

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density2d}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }
\StringTok{    }\NormalTok{theta1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-143-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_density}\NormalTok{(}\OperatorTok{~}\NormalTok{(theta2 }\OperatorTok{-}\StringTok{ }
\StringTok{    }\NormalTok{theta1))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-143-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{difference =}\NormalTok{ theta2 }\OperatorTok{-}\StringTok{ }
\StringTok{    }\NormalTok{theta1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(difference) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{acf}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-143-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{, step}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }
\StringTok{    }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }
    \DataTypeTok{angle =} \DecValTok{30}\NormalTok{, }\DataTypeTok{length =} \KeywordTok{unit}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\StringTok{"inches"}\NormalTok{))) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Notas-Curso-Estadistica_files/figure-latex/unnamed-chunk-144-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Gibbsplot2 <-}\StringTok{ }\NormalTok{Gibbs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(step }\OperatorTok{<}\StringTok{ }\DecValTok{500}\NormalTok{, step}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }
\StringTok{    }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gf_path}\NormalTok{(theta2 }\OperatorTok{~}\StringTok{ }\NormalTok{theta1, }\DataTypeTok{color =} \OperatorTok{~}\NormalTok{step, }
    \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{arrow =} \KeywordTok{arrow}\NormalTok{(}\DataTypeTok{type =} \StringTok{"open"}\NormalTok{, }\DataTypeTok{angle =} \DecValTok{30}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{transition_reveal}\NormalTok{(step)}

\KeywordTok{animate}\NormalTok{(Gibbsplot2, }\DataTypeTok{fps =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\printbibliography

\end{document}
