
@book{Albert2009,
  title = {Bayesian Computation with {{R}}},
  author = {Albert, Jim and Gentleman, Robert and Parmigiani, Giovanni and Hornik, Kurt},
  year = {2009},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-92298-0},
  abstract = {There has been a dramatic growth in the development and application of Bayesian inferential methods. Some of this growth is due to the availability of powerful simulation-based algorithms to summarize posterior distributions. There has been also a growing interest in the use of the system R for statistical analyses. R's open source nature, free availability, and large number of contributor packages have made R the software of choice for many statisticians in education and industry. Bayesian Computation with R introduces Bayesian modeling by the use of computation using the R language. The early chapters present the basic tenets of Bayesian thinking by use of familiar one and two-parameter inferential problems. Bayesian computational methods such as Laplace's method, rejection sampling, and the SIR algorithm are illustrated in the context of a random effects model. The construction and implementation of Markov Chain Monte Carlo (MCMC) methods is introduced. These simulation-based algorithms are implemented for a variety of Bayesian applications such as normal and binary response regression, hierarchical modeling, order-restricted inference, and robust modeling. Algorithms written in R are used to develop Bayesian tests and assess Bayesian models by use of the posterior predictive distribution. The use of R to interface with WinBUGS, a popular MCMC computing language, is described with several illustrative examples. This book is a suitable companion book for an introductory course on Bayesian methods and is valuable to the statistical practitioner who wishes to learn more about the R language and Bayesian methodology. The LearnBayes package, written by the author and available from the CRAN website, contains all of the R functions described in the book. The second edition contains several new topics such as the use of mixtures of conjugate priors and the use of Zellner's g priors to choose between models in linear regression. There are more illustrations of the construction of informative prior distributions, such as the use of conditional means priors and multivariate normal priors in binary regressions. The new edition contains changes in the R code illustrations according to the latest edition of the LearnBayes package. Jim Albert is Professor of Statistics at Bowling Green State University. He is Fellow of the American Statistical Association and is past editor of The American Statistician. His books include Ordinal Data Modeling (with Val Johnson), Workshop Statistics: Discovery with Data, A Bayesian Approach (with Allan Rossman), and Bayesian Computation using Minitab. \textcopyright{} 2009 Springer Science+Business Media, LLC. All rights reserved.},
  isbn = {978-0-387-92297-3},
  journal = {Bayesian Computation with R}
}

@article{CavanaughAkaike2019,
  title = {The {{Akaike}} Information Criterion: {{Background}}, Derivation, Properties, Application, Interpretation, and Refinements},
  shorttitle = {The {{Akaike}} Information Criterion},
  author = {Cavanaugh, Joseph E. and Neath, Andrew A.},
  year = {2019},
  volume = {11},
  pages = {e1460},
  issn = {1939-0068},
  doi = {10.1002/wics.1460},
  abstract = {The Akaike information criterion (AIC) is one of the most ubiquitous tools in statistical modeling. The first model selection criterion to gain widespread acceptance, AIC was introduced in 1973 by Hirotugu Akaike as an extension to the maximum likelihood principle. Maximum likelihood is conventionally applied to estimate the parameters of a model once the structure and dimension of the model have been formulated. Akaike's seminal idea was to combine into a single procedure the process of estimation with structural and dimensional determination. This article reviews the conceptual and theoretical foundations for AIC, discusses its properties and its predictive interpretation, and provides a synopsis of important practical issues pertinent to its application. Comparisons and delineations are drawn between AIC and its primary competitor, the Bayesian information criterion (BIC). In addition, the article covers refinements of AIC for settings where the asymptotic conditions and model specification assumptions that underlie the justification of AIC may be violated. This article is categorized under: Software for Computational Statistics {$>$} Artificial Intelligence and Expert Systems Statistical Models {$>$} Model Selection Statistical and Graphical Methods of Data Analysis {$>$} Modeling Methods and Algorithms Statistical and Graphical Methods of Data Analysis {$>$} Information Theoretic Methods},
  file = {/home/maikol/Zotero/storage/ESGJX7Y3/Cavanaugh and Neath - 2019 - The Akaike information criterion Background, deri.pdf;/home/maikol/Zotero/storage/3PND5PC6/wics.html},
  journal = {WIREs Computational Statistics},
  keywords = {AIC,Kullbackâ€“Leibler information,model selection criterion},
  language = {en},
  number = {3}
}

@article{Efron1979,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  author = {Efron, B.},
  year = {1979},
  month = jan,
  volume = {7},
  pages = {1--26},
  issn = {0090-5364},
  doi = {10.1214/aos/1176344552},
  abstract = {We discuss the following problem: given a random sample X=(X1,X2,{$\cdots$},Xn)X=(X1,X2,{$\cdots$},Xn)\textbackslash mathbf\{X\} = (X\_1, X\_2, \textbackslash cdots, X\_n) from an unknown probability distribution FFF, estimate the sampling distribution of some prespecified random variable R(X,F)R(X,F)R(\textbackslash mathbf\{X\}, F), on the basis of the observed data xx\textbackslash mathbf\{x\}. (Standard jackknife theory gives an approximate mean and variance in the case R(X,F)=\texttheta (F\^)-\texttheta (F),\texttheta R(X,F)=\texttheta (F\^)-\texttheta (F),\texttheta R(\textbackslash mathbf\{X\}, F) = \textbackslash theta(\textbackslash hat\{F\}) - \textbackslash theta(F), \textbackslash theta some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
  journal = {The Annals of Statistics},
  number = {1}
}

@article{Hall1987,
  title = {On {{Kullback}}-{{Leibler Loss}} and {{Density Estimation}}},
  author = {Hall, Peter},
  year = {1987},
  month = dec,
  volume = {15},
  pages = {1491--1519},
  issn = {0090-5364},
  doi = {10.1214/aos/1176350606},
  abstract = {4, 1491-1519 ON -LEIBLER LOSS AND DENSITY ESTIMATION By Peter Hall Australian National University "Discrimination information," or -Leibler loss},
  journal = {The Annals of Statistics},
  number = {4}
}

@book{HardleNonparametric2004,
  title = {Nonparametric and {{Semiparametric Models}}},
  author = {H{\"a}rdle, Wolfgang and Werwatz, Axel and M{\"u}ller, Marlene and Sperlich, Stefan},
  year = {2004},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17146-8},
  isbn = {978-3-642-62076-8}
}

@book{HastieElements2009,
  ids = {Hastie2009,Hastie2009a},
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data}} Mining, {{Inference}}, and {{Prediction}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {01727397},
  doi = {10.1007/978-0-387-84858-7},
  abstract = {"During the past decade there has been an explosion in computation
and information technology. With it have come vast amounts of data
in a variety of fields such as medicine, biology, finance, and marketing.
The challenge of understanding these data has led to the development
of new tools in the field of statistics, and spawned new areas such
as data mining, machine learning, and bioinformatics. Many of these
tools have common underpinnings but are often expressed with different
terminology. This book describes the important ideas in these areas
in a common conceptual framework. While the approach is statistical,
the emphasis is on concepts rather than mathematics. Many examples
are given, with a liberal use of color graphics."--Jacket.},
  isbn = {978-0-387-84857-0},
  pmid = {15512507}
}

@article{Hoffman2014,
  title = {The No-{{U}}-Turn Sampler: {{Adaptively}} Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  volume = {15},
  pages = {1593--1623},
  issn = {15337928},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size e and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter e on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers. \textcopyright{} 2014 Matthew D. Hoffman and Andrew Gelman.},
  archivePrefix = {arXiv},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  journal = {Journal of Machine Learning Research},
  keywords = {Adaptive Monte Carlo,Bayesian inference,Dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo},
  number = {47}
}

@book{HussonExploratory2017,
  title = {Exploratory {{Multivariate Analysis}} by {{Example Using R}}},
  author = {Husson, Francois and Le, Sebastien and Pag{\`e}s, J{\'e}r{\^o}me},
  year = {2017},
  month = apr,
  publisher = {{CRC Press}},
  abstract = {Full of real-world case studies and practical advice, Exploratory Multivariate Analysis by Example Using R, Second Edition focuses on four fundamental methods of multivariate exploratory data analysis that are most suitable for applications. It covers principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) a},
  googlebooks = {nLrODgAAQBAJ},
  isbn = {978-1-315-30186-0},
  keywords = {Mathematics / Probability \& Statistics / General},
  language = {en}
}

@book{James2013b,
  ids = {James2013a,JamesIntroduction2013},
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  isbn = {978-1-4614-7137-0}
}

@incollection{Kruschke2014,
  title = {Doing {{Bayesian}} Data Analysis: {{A}} Tutorial with {{R}}, {{JAGS}}, and {{Stan}}, Second Edition},
  booktitle = {Doing {{Bayesian Data Analysis}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}, {{Second Edition}}},
  author = {Kruschke, John K.},
  year = {2014},
  pages = {1--759},
  abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
  isbn = {978-0-12-405888-0}
}

@article{Quenouille1949,
  title = {Approximate {{Tests}} of {{Correlation}} in {{Time}}-{{Series}}},
  author = {Quenouille, M. H.},
  year = {1949},
  month = jan,
  volume = {11},
  pages = {68--84},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1949.tb00023.x},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  number = {1}
}

@article{StoneAsymptotic1977,
  ids = {StoneAsymptotic1977a,StoneAsymptotic1977b},
  title = {An {{Asymptotic Equivalence}} of {{Choice}} of {{Model}} by {{Cross}}-{{Validation}} and {{Akaike}}'s {{Criterion}}},
  author = {Stone, M.},
  year = {1977},
  volume = {39},
  pages = {44--47},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
  file = {/home/maikol/Zotero/storage/KFNDE6SH/Stone - 1977 - An Asymptotic Equivalence of Choice of Model by Cr.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {1}
}

@book{Wasserman2006,
  title = {All of {{Nonparametric Statistics}}},
  author = {Wasserman, Larry},
  year = {2006},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-30623-4},
  abstract = {Aimed at Masters or PhD level students in statistics, computer science, and engineering, this comprehensive text provides the reader with a single book where ...},
  isbn = {978-0-387-25145-5},
  journal = {All of Nonparametric Statistics}
}


