@book{Hardle2004,
address = {Berlin, Heidelberg},
author = {H{\"{a}}rdle, Wolfgang and Werwatz, Axel and M{\"{u}}ller, Marlene and Sperlich, Stefan},
doi = {10.1007/978-3-642-17146-8},
isbn = {978-3-642-62076-8},
pages = {xxviii+299},
publisher = {Springer Berlin Heidelberg},
series = {Springer Series in Statistics},
title = {{Nonparametric and Semiparametric Models}},
url = {http://link.springer.com/10.1007/978-3-642-17146-8},
year = {2004}
}
@article{Hall1987,
abstract = {4, 1491-1519 ON -LEIBLER LOSS AND DENSITY ESTIMATION By Peter Hall Australian National University "Discrimination information," or -Leibler loss},
author = {Hall, Peter},
doi = {10.1214/aos/1176350606},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {dec},
number = {4},
pages = {1491--1519},
title = {{On Kullback-Leibler Loss and Density Estimation}},
url = {http://projecteuclid.org/euclid.aos/1176350606},
volume = {15},
year = {1987}
}
@article{Efron1979,
abstract = {We discuss the following problem: given a random sample X=(X1,X2,⋯,Xn)X=(X1,X2,⋯,Xn)\mathbf{X} = (X_1, X_2, \cdots, X_n) from an unknown probability distribution FFF, estimate the sampling distribution of some prespecified random variable R(X,F)R(X,F)R(\mathbf{X}, F), on the basis of the observed data xx\mathbf{x}. (Standard jackknife theory gives an approximate mean and variance in the case R(X,F)=$\theta$(F^)−$\theta$(F),$\theta$R(X,F)=$\theta$(F^)−$\theta$(F),$\theta$R(\mathbf{X}, F) = \theta(\hat{F}) - \theta(F), \theta some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
author = {Efron, B.},
doi = {10.1214/aos/1176344552},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {jan},
number = {1},
pages = {1--26},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
url = {http://projecteuclid.org/euclid.aos/1176344552},
volume = {7},
year = {1979}
}
@article{Quenouille1949,
author = {Quenouille, M. H.},
doi = {10.1111/j.2517-6161.1949.tb00023.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {jan},
number = {1},
pages = {68--84},
title = {{Approximate Tests of Correlation in Time-Series}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1949.tb00023.x},
volume = {11},
year = {1949}
}
@incollection{Kruschke2014,
abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
author = {Kruschke, John K.},
booktitle = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition},
isbn = {9780124058880},
pages = {1--759},
title = {{Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, second edition}},
year = {2014}
}
