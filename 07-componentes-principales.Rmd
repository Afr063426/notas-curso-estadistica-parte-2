# Análisis en componentes principales

## Aprendizaje no-supervisado

Al contrario de los métodos que se han estudiado de regresión y clasificación, en este caso no hay variable dependiente, y el conjunto de datos está compuesto de $p$ variables o características y $n$ observaciones. 

El principal objetivo del aprendizaje no-supervisado no es la predicción, sino en el *análisis de datos* por sí mismo, es decir se quiere buscar patrones o relaciones interesantes dentro de la tabla de datos: por ejemplo la visualización de datos o la identificación de subgrupos en los datos. (Análisis Exploratorio de Datos)

En el caso de aprendizaje no-supervisado, no es posible verificar o validar los métodos adoptados.


Si se quiere seleccionar la mejor proyección de 2 variables de una nube de puntos $X_1,\dots, X_p$, se debe hacer $\binom{p}{2}$ gráficos de dispersión. Un criterio de búsqueda es seleccionar la que tenga mayor información, en el sentido de mayor variabilidad.

Usaremos como base los libros de [@HussonExploratory2017] y [@James2013b].

```{r}
library(rgl)
library(car)
knitr::knit_hooks$set(webgl = hook_webgl, rgl = hook_rgl)
knitr::opts_chunk$set(fig.pos = "!h")
```

```{r}
set.seed(123)
x1 <- rnorm(1000, 0, 2)
x2 <- cos(rnorm(1000, 0, 2))
x3 <- x1 + rnorm(1000, 0, 2)
```

```{r}
GGally::ggpairs(data.frame(x1,x2,x3))
```


```{r,webgl=TRUE,echo=knitr::is_html_output()}
plot3d(x1,
          x2,
          x3,
          point.col = "black")

plot3d(scale(x1),
       scale(x2),
       scale(x3),
       point.col = "black")

```

```{r,rgl=TRUE,echo=knitr::is_latex_output()}

plot3d(
  x1,
  x3,
  x2,
  point.col = "black"
)

plot3d(scale(x1),
       scale(x2),
       scale(x3),
       point.col = "black")

```




<!-- ```{r, echo=FALSE} -->
<!-- library(ggplot2) -->

<!-- qplot(1:300, rnorm(300, sd = 0.1) + 5, ylim = c(0, 10)) + theme_minimal() + xlab("") + ylab("") -->
<!-- qplot(1:300, (3 * 1:300 + 100 * cos(1:300 / (2 * pi)) + 200 * rnorm(300, sd = -->
<!--                                                                       0.1)) / 100) + theme_minimal() + xlab("") + ylab("") -->
<!-- ``` -->

El ACP lo que busca es un número reducido de dimensión que represente el máximo de variabilidad en las observaciones eliminando la mayor cantidad de ruido posible. 

## Representación gráfica

![Tomado de [The shape of data](https://shapeofdata.wordpress.com/2013/04/09/principle-component-analysis/)](manual_figures/pca.png)

## Primer componente principal





$$ Z_1 := \phi_{11}x_1 +  \phi_{21}x_2 + \dots + \phi_{p1}x_p;\quad \text{con } \sum_{j=1}^{p}\phi_{j1} = 1$$
tal que $Z_1$ tenga la varianza máxima.

Al vector $\phi_1 = (\phi_{11}, \phi_{21},\dots,\phi_{p1})$ se le llama *pasos o cargas*. 

$X = (X_1,\dots,X_p)_{n\times p}$ es la *matriz de diseño* donde cada columna tiene media 0. Se resuelve el problema
$$\hat{\phi}_1=\underset{\Vert\phi_1\Vert_2^2=1}{\mathrm{argmax}} \left\lbrace\dfrac{1}{n}\sum_{i=1}^{n}\left(\sum_{i=1}^p \phi_{j1} X_{ij} \right)^2 \right\rbrace $$
La restricción de minimización se puede rescribir como $\Vert\phi_1\Vert_2^2= \sum_{j=1}^p \phi_{j1}^2 = 1$

Los $Z_{11},\dots, Z_{n1}$ son los scores del primer componente principal.

$\phi_1$ es la dirección en el espacio característico en $\mathbb{R}^p$ en donde los datos tengan la máxima varianza.


Esta última expresión se podría rescribir de forma matricial como 

\[
\hat{\phi}_1 = \underset{\Vert\phi_1\Vert_2^2=1}{\mathrm{argmax}} \left\{ \phi_1^\top X^\top X \phi_1 \right\}
\]

donde $\phi_1 = (\phi_{11}, \phi_{21},\dots,\phi_{p1})$


dadas las condiciones, esta expresión se podría simplificar un poco más en 

\[
\hat{\phi}_1 = \underset{\phi_1}{\mathrm{argmax}} \left\{\frac{\phi_1^\top X^\top X \phi_1 }{\phi_1^\top \phi_1}\right\}
\]

Dado que la expresión anterio es un coeficiente de Rayleigh, se puede probar  que \(\hat{\phi}_{1}\) corresponde al primer vector propio de la matriz $X^\top X = \mathrm{Cov}(X)$ si las columnas de $X$ son centradas.

## Segunda componente principal



$$ Z_{2}:= \phi_{12}x_1 + \phi_{22}x_2+\dots+\phi_{p2}x_p$$
 $$\underset{\Vert\phi_2\Vert_2^2=1}{\mathrm{argmax}} \left\lbrace\dfrac{1}{n}\sum_{i=1}^{n}\left(\sum_{i=1}^p \phi_{j2} X_{ij} \right)^2 \right\rbrace$$
 Se tiene, además, que $\forall i$, $Z_{i2}\perp Z_1$, entonces 
 $$ Z_{i2}\perp Z_1 \implies \phi_{2} \perp \phi_{1}$$

Esto se logra primero construyendo una matriz nueva de diseño, restando a la matrix $X$ original, el primer componente principal. 

\[
\tilde{X}_2 = X - X\phi_1\phi_1^\top
\]

Luego a esa matriz, se le aplica el procedimiento anterior 

\[
\hat{\phi}_2 = \underset{\phi_2}{\mathrm{argmax}} \left\{\frac{\phi_2^\top X^\top X \phi_2 }{\phi_2^\top \phi_2}\right\}
\]

Y nuevamente se puede probar que el componente principal corresponde al segundo vector propio de 
$X^\top X = \mathrm{Cov}(X)$


De la misma forma se construye $\phi_3,\phi_4,\dots, \phi_p$.

Notas: 

- **Escalas**: la varianza de las variables depende de las unidades. El problema es que los pesos $\phi_i$ son distintos dependiendo de las escalas. La solución es estandarizar las variables: $\dfrac{X_i-\mu_i}{\hat\sigma_i}$.
- **Unicidad**: los componentes principales son únicos, módulo cambio de signo.
\end{itemize}


## Circulo de correlaciones 

Se puede construir la correlación de cada variable con respecto a cada componente principal 

\[
cos(\theta_{i,j^\prime}) = \mathrm{Corr}(X_i, \mathrm{PC}_{j^\prime})
\]

El ángulo $\theta_{i,j^\prime}$ significa la lejanía o cercanía de cierta variable con respecto a cada componente principal. 

Además, basados en el el círculo identidad $\cos^2(\theta)+\sin^2(\theta)=1$, el valor de $cos^2(\theta_{i,j^\prime})$ representa la "intensidad" con la cual la variable $X_i$ es representada por el componente principal $\mathrm{PC}_{i^\prime}$.

## Volvamos a nuestro ejemplo 

```{r}
library("factoextra")
library("FactoMineR")
p <- PCA(scale(cbind(x1,x2,x3)))
```

```{r}
p$var$cor
p$var$cos2
```





## ¿Cuántos componentes usar?

```{r}
fviz_screeplot(p, addlabels = F, ylim = c(0, 50)) + xlab("Variables") + ylab("Porcentaje de varianza de Z explicada") + labs(title = "Diagrama")
```
```{r}
qplot(1:3, p$eig[, 3], geom = "point") + xlab("Cantidad de componentes") + ylab("Varianza acumulada") + geom_line() + theme_minimal() + geom_hline(yintercept = 80, color = "red") +
  scale_x_continuous(breaks = 1:10)
```






## Laboratorio

Vamos a usar los datos `decathlon` de `FactomineR` que representa los resultados de varios atletas en pruebas de decathlon en el 2004. 

El objetivo es encontrar si hay patrones entre ciudad y tipos de crimen.

Exploración de datos
Ejecute una exploración de datos


```{r,message=F,echo=F}
data("decathlon")
summary(decathlon)
```


```{r,message=F,echo=F}
acp.decathlon <- PCA(decathlon, quanti.sup = 11:12, quali.sup = 13)
```


```{r,message=F,echo=F}
fviz_screeplot(acp.decathlon, addlabels = F, ylim = c(0, 50)) + xlab("Variables") + ylab("Porcentaje de varianza de Z explicada") + labs(title = "Diagrama")
```


```{r,message=F,echo=F}
pvc <- acp.decathlon$eig[, 3]
```


```{r,message=F,echo=F}
qplot(1:10, pvc, geom = "point") + xlab("Cantidad de componentes") + ylab("Varianza acumulada") + geom_line() + theme_minimal() + geom_hline(yintercept = 80, color = "red") +
  scale_x_continuous(breaks = 1:10)
```


```{r}
plot(acp.decathlon$ind$coord[, 1],
       acp.decathlon$ind$coord[, 2])

plot(acp.decathlon$ind$coord[, 3],
       acp.decathlon$ind$coord[, 4])

```

```

## Ejercicios 

- Del libro [@James2013b] 
    - Capítulo 10:  6, 8
`

